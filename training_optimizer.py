#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ SmartFarm Training Optimizer - Colab & Early Stopping Integration

YOLOv8/YOLO11 eÄŸitimi iÃ§in Google Colab optimize edilmiÅŸ training sistemi.
Early stopping, epoch tahmini ve optimal parametre Ã¶nerileri iÃ§erir.

Ã–zellikler:
- Colab kaynak optimizasyonu
- Otomatik early stopping
- Epoch sÃ¼resi tahmini
- Optimal epoch sayÄ±sÄ± hesaplama
- Memory management
- Progress tracking

KullanÄ±m:
    from training_optimizer import SmartTrainingOptimizer
    
    optimizer = SmartTrainingOptimizer()
    config = optimizer.get_optimal_training_config(dataset_size=5000)
"""

import os
import psutil
import torch
import numpy as np
from datetime import datetime
import yaml
from pathlib import Path
from config_utils import get_default_batch_size, get_training_config_from_yaml
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import numpy as np

# SmartFarm modÃ¼lleri
try:
    from early_stopping_system import EarlyStoppingManager, EarlyStoppingConfig, TrainingMetrics
    from colab_optimized_validator import ColabAugmentationValidator, get_colab_resources
    SMARTFARM_MODULES_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ SmartFarm modÃ¼lleri import edilemedi: {e}")
    SMARTFARM_MODULES_AVAILABLE = False

# Colab tespiti
def is_colab():
    try:
        import google.colab
        return True
    except ImportError:
        return False

# GPU kontrolÃ¼
def check_gpu_availability():
    """GPU durumunu kontrol et"""
    gpu_info = {
        'available': False,
        'name': 'CPU',
        'memory_gb': 0,
        'compute_capability': None
    }
    
    try:
        import torch
        if torch.cuda.is_available():
            gpu_info['available'] = True
            gpu_info['name'] = torch.cuda.get_device_name(0)
            gpu_info['memory_gb'] = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            gpu_info['compute_capability'] = torch.cuda.get_device_capability(0)
    except ImportError:
        pass
    
    return gpu_info


class SmartTrainingOptimizer:
    """AkÄ±llÄ± eÄŸitim optimizasyonu sistemi"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.colab_resources = get_colab_resources() if SMARTFARM_MODULES_AVAILABLE else {}
        self.gpu_info = check_gpu_availability()
        
        print("ğŸ¯ SmartFarm Training Optimizer baÅŸlatÄ±ldÄ±")
        print(f"ğŸŒ Ortam: {'Google Colab' if is_colab() else 'Yerel'}")
        print(f"ğŸ® GPU: {self.gpu_info['name']}")
        if self.gpu_info['available']:
            print(f"ğŸ’¾ GPU Memory: {self.gpu_info['memory_gb']:.1f}GB")
    
    def analyze_dataset(self, images_dir: str, labels_dir: str) -> Dict[str, Any]:
        """Dataset analizi"""
        images_path = Path(images_dir)
        labels_path = Path(labels_dir)
        
        # GÃ¶rÃ¼ntÃ¼ dosyalarÄ±nÄ± say
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
        image_files = []
        for ext in image_extensions:
            image_files.extend(list(images_path.glob(f'*{ext}')))
            image_files.extend(list(images_path.glob(f'*{ext.upper()}')))
        
        # Etiket dosyalarÄ±nÄ± say
        label_files = list(labels_path.glob('*.txt'))
        
        # Ã–rnek gÃ¶rÃ¼ntÃ¼ boyutu analizi
        sample_sizes = []
        if image_files:
            try:
                import cv2
                for img_file in image_files[:10]:  # Ä°lk 10 dosyayÄ± kontrol et
                    img = cv2.imread(str(img_file))
                    if img is not None:
                        h, w = img.shape[:2]
                        sample_sizes.append((w, h))
            except ImportError:
                pass
        
        # Ortalama gÃ¶rÃ¼ntÃ¼ boyutu
        if sample_sizes:
            avg_width = np.mean([s[0] for s in sample_sizes])
            avg_height = np.mean([s[1] for s in sample_sizes])
            avg_pixels = avg_width * avg_height
        else:
            avg_width = avg_height = avg_pixels = 0
        
        # SÄ±nÄ±f analizi (etiket dosyalarÄ±ndan)
        class_counts = {}
        total_objects = 0
        
        for label_file in label_files[:100]:  # Ä°lk 100 dosyayÄ± analiz et
            try:
                with open(label_file, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if parts:
                            class_id = int(parts[0])
                            class_counts[class_id] = class_counts.get(class_id, 0) + 1
                            total_objects += 1
            except:
                continue
        
        return {
            'total_images': len(image_files),
            'total_labels': len(label_files),
            'matched_pairs': min(len(image_files), len(label_files)),
            'avg_image_size': {
                'width': int(avg_width) if avg_width > 0 else 640,
                'height': int(avg_height) if avg_height > 0 else 640,
                'pixels': int(avg_pixels) if avg_pixels > 0 else 640*640
            },
            'num_classes': len(class_counts),
            'total_objects': total_objects,
            'avg_objects_per_image': total_objects / max(1, len(label_files)),
            'class_distribution': class_counts
        }
    
    def get_optimal_training_config(self, 
                                  dataset_size: int,
                                  model_size: str = "yolov8n",
                                  task_complexity: str = "medium",
                                  target_accuracy: float = 0.8) -> Dict[str, Any]:
        """Optimal eÄŸitim konfigÃ¼rasyonu hesapla"""
        
        # Model boyutu kategorisi
        if 'n' in model_size.lower():
            model_category = "nano"
        elif 's' in model_size.lower():
            model_category = "small"
        elif 'm' in model_size.lower():
            model_category = "medium"
        elif 'l' in model_size.lower():
            model_category = "large"
        elif 'x' in model_size.lower():
            model_category = "xlarge"
        else:
            model_category = "medium"
        
        # Base konfigÃ¼rasyon
        # Config dosyasÄ±ndan batch size al
        config_batch_size = get_default_batch_size()
        
        base_configs = {
            "nano": {
                "base_epochs": {"simple": 100, "medium": 200, "complex": 300},
                "batch_size": {"colab": config_batch_size, "local": config_batch_size},
                "image_size": 640,
                "patience": 30
            },
            "small": {
                "base_epochs": {"simple": 150, "medium": 250, "complex": 400},
                "batch_size": {"colab": config_batch_size, "local": config_batch_size},
                "image_size": 640,
                "patience": 40
            },
            "medium": {
                "base_epochs": {"simple": 200, "medium": 300, "complex": 500},
                "batch_size": {"colab": config_batch_size, "local": config_batch_size},
                "image_size": 640,
                "patience": 50
            },
            "large": {
                "base_epochs": {"simple": 250, "medium": 400, "complex": 600},
                "batch_size": {"colab": config_batch_size, "local": config_batch_size},
                "image_size": 640,
                "patience": 60
            },
            "xlarge": {
                "base_epochs": {"simple": 300, "medium": 500, "complex": 800},
                "batch_size": {"colab": config_batch_size, "local": config_batch_size},
                "image_size": 640,
                "patience": 70
            }
        }
        
        config = base_configs[model_category]
        environment = "colab" if is_colab() else "local"
        
        # Dataset boyutuna gÃ¶re ayarlama
        if dataset_size < 500:
            size_factor = 0.5
            warning = "âš ï¸ Ã‡ok kÃ¼Ã§Ã¼k dataset - overfitting riski Ã§ok yÃ¼ksek!"
            recommendation = "Daha fazla veri toplayÄ±n veya augmentation kullanÄ±n"
        elif dataset_size < 1000:
            size_factor = 0.7
            warning = "âš ï¸ KÃ¼Ã§Ã¼k dataset - overfitting riski yÃ¼ksek"
            recommendation = "Early stopping ve augmentation kullanÄ±n"
        elif dataset_size < 5000:
            size_factor = 1.0
            warning = None
            recommendation = "Dengeli dataset boyutu"
        elif dataset_size < 20000:
            size_factor = 1.3
            warning = None
            recommendation = "Ä°yi dataset boyutu - uzun eÄŸitim yapabilirsiniz"
        else:
            size_factor = 1.5
            warning = None
            recommendation = "BÃ¼yÃ¼k dataset - daha uzun eÄŸitim gerekebilir"
        
        # HesaplanmÄ±ÅŸ deÄŸerler
        base_epochs = config["base_epochs"][task_complexity]
        recommended_epochs = int(base_epochs * size_factor)
        
        # Colab iÃ§in Ã¶zel ayarlamalar
        if is_colab():
            # Colab session timeout (12 saat) gÃ¶z Ã¶nÃ¼nde bulundur
            max_colab_epochs = 800  # GÃ¼venli Ã¼st limit
            recommended_epochs = min(recommended_epochs, max_colab_epochs)
            
            # Batch size config dosyasÄ±ndan okunuyor
            config["batch_size"][environment] = get_default_batch_size()
        
        # Early stopping konfigÃ¼rasyonu
        early_stopping_config = {
            "patience": max(20, recommended_epochs // 10),
            "min_delta": 0.001,
            "monitor_metric": "val_loss",
            "overfitting_threshold": 0.1 if dataset_size < 1000 else 0.15
        }
        
        # Epoch sÃ¼resi tahmini
        estimated_epoch_duration = self._estimate_epoch_duration(
            dataset_size, config["batch_size"][environment], 
            config["image_size"], model_category
        )
        
        total_estimated_time = recommended_epochs * estimated_epoch_duration
        
        # 2000 epoch analizi
        epoch_2000_analysis = self._analyze_2000_epochs(
            dataset_size, model_category, task_complexity
        )
        
        return {
            "recommended_config": {
                "epochs": recommended_epochs,
                "min_epochs": max(50, recommended_epochs // 3),
                "max_epochs": min(1000, recommended_epochs * 2),
                "batch_size": config["batch_size"][environment],
                "image_size": config["image_size"],
                "patience": early_stopping_config["patience"],
                "learning_rate": 0.01,
                "weight_decay": 0.0005
            },
            "early_stopping": early_stopping_config,
            "time_estimates": {
                "estimated_epoch_duration_minutes": estimated_epoch_duration / 60,
                "total_estimated_hours": total_estimated_time / 3600,
                "colab_session_warning": total_estimated_time > 10 * 3600 if is_colab() else False
            },
            "dataset_analysis": {
                "size": dataset_size,
                "size_factor": size_factor,
                "category": "KÃ¼Ã§Ã¼k" if dataset_size < 1000 else "Orta" if dataset_size < 10000 else "BÃ¼yÃ¼k"
            },
            "model_info": {
                "model_size": model_size,
                "category": model_category,
                "complexity": task_complexity
            },
            "warnings": [w for w in [warning] if w],
            "recommendations": [recommendation],
            "epoch_2000_analysis": epoch_2000_analysis,
            "environment": {
                "platform": "Google Colab" if is_colab() else "Local",
                "gpu_available": self.gpu_info['available'],
                "gpu_memory_gb": self.gpu_info['memory_gb']
            }
        }
    
    def _estimate_epoch_duration(self, dataset_size: int, batch_size: int, 
                               image_size: int, model_category: str) -> float:
        """Epoch sÃ¼resi tahmini (saniye)"""
        
        # Base duration per image (saniye)
        base_durations = {
            "nano": 0.01,
            "small": 0.015,
            "medium": 0.025,
            "large": 0.04,
            "xlarge": 0.06
        }
        
        base_duration = base_durations.get(model_category, 0.025)
        
        # GPU faktÃ¶rÃ¼
        gpu_factor = 0.3 if self.gpu_info['available'] else 1.0
        
        # Colab faktÃ¶rÃ¼ (shared resources)
        colab_factor = 1.2 if is_colab() else 1.0
        
        # Image size faktÃ¶rÃ¼
        size_factor = (image_size / 640) ** 2
        
        # Toplam sÃ¼re hesapla
        images_per_epoch = dataset_size
        batches_per_epoch = np.ceil(images_per_epoch / batch_size)
        
        epoch_duration = (batches_per_epoch * base_duration * 
                         gpu_factor * colab_factor * size_factor)
        
        return max(30, epoch_duration)  # Minimum 30 saniye
    
    def _analyze_2000_epochs(self, dataset_size: int, model_category: str, 
                           task_complexity: str) -> Dict[str, Any]:
        """2000 epoch analizi"""
        
        # 2000 epoch'un mantÄ±klÄ± olup olmadÄ±ÄŸÄ±nÄ± analiz et
        optimal_range = {
            "nano": {"simple": (50, 300), "medium": (100, 500), "complex": (200, 800)},
            "small": {"simple": (100, 400), "medium": (150, 600), "complex": (250, 1000)},
            "medium": {"simple": (150, 500), "medium": (200, 800), "complex": (300, 1200)},
            "large": {"simple": (200, 600), "medium": (250, 1000), "complex": (400, 1500)},
            "xlarge": {"simple": (250, 800), "medium": (300, 1200), "complex": (500, 2000)}
        }
        
        min_optimal, max_optimal = optimal_range[model_category][task_complexity]
        
        # Dataset boyutu faktÃ¶rÃ¼
        if dataset_size < 1000:
            max_optimal = min(max_optimal, 500)  # KÃ¼Ã§Ã¼k dataset iÃ§in limit
        
        is_reasonable = min_optimal <= 2000 <= max_optimal * 1.5
        
        if 2000 > max_optimal * 2:
            verdict = "Ã‡ok Fazla"
            reason = f"2000 epoch bu konfigÃ¼rasyon iÃ§in Ã§ok fazla. Ã–nerilen maksimum: {max_optimal}"
            risk = "YÃ¼ksek overfitting riski, zaman kaybÄ±"
        elif 2000 > max_optimal:
            verdict = "Fazla"
            reason = f"2000 epoch biraz fazla olabilir. Ã–nerilen aralÄ±k: {min_optimal}-{max_optimal}"
            risk = "Orta overfitting riski"
        elif 2000 >= min_optimal:
            verdict = "Makul"
            reason = f"2000 epoch bu konfigÃ¼rasyon iÃ§in makul. Early stopping kullanÄ±n."
            risk = "DÃ¼ÅŸÃ¼k risk (early stopping ile)"
        else:
            verdict = "Az"
            reason = f"2000 epoch bu model iÃ§in az olabilir. Minimum Ã¶nerilen: {min_optimal}"
            risk = "Underfitting riski"
        
        return {
            "verdict": verdict,
            "is_reasonable": is_reasonable,
            "reason": reason,
            "risk_assessment": risk,
            "optimal_range": f"{min_optimal}-{max_optimal}",
            "recommendation": f"Early stopping ile {min_optimal}-{max_optimal*2} aralÄ±ÄŸÄ±nda baÅŸlayÄ±n"
        }
    
    def create_training_script(self, config: Dict[str, Any], 
                             output_path: str = "optimized_training.py") -> str:
        """Optimize edilmiÅŸ eÄŸitim scripti oluÅŸtur"""
        
        script_content = f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ SmartFarm Optimized Training Script
Otomatik oluÅŸturulmuÅŸ eÄŸitim scripti - {datetime.now().strftime("%Y-%m-%d %H:%M")}
"""

import os
import sys
from pathlib import Path

# SmartFarm modÃ¼llerini import et
try:
    from early_stopping_system import EarlyStoppingManager, EarlyStoppingConfig, TrainingMetrics
    from training_optimizer import SmartTrainingOptimizer
    print("âœ… SmartFarm modÃ¼lleri yÃ¼klendi")
except ImportError as e:
    print(f"âš ï¸ SmartFarm modÃ¼lleri yÃ¼klenemedi: {{e}}")
    print("Temel YOLO eÄŸitimi ile devam ediliyor...")

# YOLOv8 import
try:
    from ultralytics import YOLO
    print("âœ… Ultralytics YOLO yÃ¼klendi")
except ImportError:
    print("âŒ Ultralytics yÃ¼klenemedi! pip install ultralytics")
    sys.exit(1)

def main():
    """Ana eÄŸitim fonksiyonu"""
    
    # KonfigÃ¼rasyon
    config = {{
        "model": "{config['model_info']['model_size']}",
        "epochs": {config['recommended_config']['epochs']},
        "batch_size": {config['recommended_config']['batch_size']},
        "image_size": {config['recommended_config']['image_size']},
        "patience": {config['recommended_config']['patience']},
        "learning_rate": {config['recommended_config']['learning_rate']},
        "weight_decay": {config['recommended_config']['weight_decay']}
    }}
    
    print("ğŸš€ SmartFarm Optimized Training baÅŸlatÄ±lÄ±yor...")
    print(f"ğŸ“Š KonfigÃ¼rasyon: {{config}}")
    
    # Early stopping manager
    early_stopping_config = EarlyStoppingConfig(
        patience={config['early_stopping']['patience']},
        min_delta={config['early_stopping']['min_delta']},
        monitor_metric="{config['early_stopping']['monitor_metric']}",
        overfitting_threshold={config['early_stopping']['overfitting_threshold']}
    )
    
    early_stopping = EarlyStoppingManager(early_stopping_config)
    
    # Model yÃ¼kle
    model = YOLO(config["model"])
    
    # EÄŸitim parametreleri
    train_args = {{
        "data": "path/to/your/dataset.yaml",  # BURAYA DATASET YAML YOLUNU GÄ°RÄ°N
        "epochs": config["epochs"],
        "batch": config["batch_size"],
        "imgsz": config["image_size"],
        "lr0": config["learning_rate"],
        "weight_decay": config["weight_decay"],
        "patience": config["patience"],
        "save": True,
        "save_period": 10,
        "cache": True,
        "device": "0" if "{config['environment']['gpu_available']}" == "True" else "cpu",
        "workers": 2,
        "project": "smartfarm_training",
        "name": "optimized_run"
    }}
    
    print("ğŸ¯ EÄŸitim baÅŸlatÄ±lÄ±yor...")
    print(f"â±ï¸ Tahmini sÃ¼re: {config['time_estimates']['total_estimated_hours']:.1f} saat")
    
    # EÄŸitimi baÅŸlat
    try:
        results = model.train(**train_args)
        print("âœ… EÄŸitim baÅŸarÄ±yla tamamlandÄ±!")
        
        # SonuÃ§larÄ± kaydet
        early_stopping.save_training_report("final_training_report.json")
        
    except Exception as e:
        print(f"âŒ EÄŸitim hatasÄ±: {{e}}")
        early_stopping.save_training_report("error_training_report.json")
    
    return results

if __name__ == "__main__":
    results = main()
'''
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        print(f"ğŸ“„ Optimize edilmiÅŸ eÄŸitim scripti oluÅŸturuldu: {output_path}")
        return output_path


# YardÄ±mcÄ±: main_multi_dataset.py tarafÄ±ndan verilen options sÃ¶zlÃ¼ÄŸÃ¼nÃ¼ normalize et
def prepare_training_options(options: Dict[str, Any]) -> Dict[str, Any]:
    """
    EÄŸitimle ilgili parametreleri merkezi olarak normalize eder.
    - main_multi_dataset.py iÃ§indeki kullanÄ±cÄ± seÃ§imlerini korur
    - Eksikse dataset Ã¶nerilerinden (varsa) batch/imgsz gibi deÄŸerleri tamamlar
    - AnahtarlarÄ±n varlÄ±ÄŸÄ±nÄ± garanti eder
    """
    opts = dict(options) if isinstance(options, dict) else {}

    # VarsayÄ±lan anahtarlar ve gÃ¼venli deÄŸerler
    defaults = {
        'project': 'runs/train',
        'name': 'exp',
        'exist_ok': True,
        'use_hyp': True,
        'speed_mode': False,
        'workers': None,
    }
    for k, v in defaults.items():
        opts.setdefault(k, v)

    # Dataset Ã¶nerilerinden batch/imgsz Ã§ek (varsa ve kullanÄ±cÄ± belirtmediyse)
    try:
        ds_cfg = opts.get('dataset_config') or {}
        if ds_cfg.get('type') == 'hierarchical_multi':
            rec = (ds_cfg.get('setup') or {}).get('recommendations') or {}
            if 'batch' not in opts or opts.get('batch') in (None, 0):
                if isinstance(rec.get('batch_size'), int) and rec['batch_size'] > 0:
                    opts['batch'] = rec['batch_size']
            if 'imgsz' not in opts or opts.get('imgsz') in (None, 0):
                if isinstance(rec.get('image_size'), int) and rec['image_size'] > 0:
                    opts['imgsz'] = rec['image_size']
    except Exception:
        pass

    # Zorunlu alanlar kontrolÃ¼ (model, data, epochs)
    required = ['model', 'data', 'epochs']
    missing = [k for k in required if k not in opts]
    if missing:
        print(f"âš ï¸ prepare_training_options: Eksik alanlar: {missing}. LÃ¼tfen interaktif kurulum adÄ±mlarÄ±nÄ± tamamlayÄ±n.")

    return opts


# KullanÄ±m Ã¶rneÄŸi ve rehber
def print_epoch_recommendations():
    """Epoch sayÄ±sÄ± rehberi yazdÄ±r"""
    
    print("""
ğŸ¯ SMARTFARM EPOCH REHBERÄ°
========================

â“ 2000 Epoch ile BaÅŸlamak MantÄ±klÄ± mÄ±?
---------------------------------------
KISA CEVAP: Genellikle HAYIR! 

ğŸ” DetaylÄ± Analiz:
â€¢ KÃ¼Ã§Ã¼k dataset (<1000 gÃ¶rÃ¼ntÃ¼): 100-300 epoch yeterli
â€¢ Orta dataset (1000-10000): 200-600 epoch optimal  
â€¢ BÃ¼yÃ¼k dataset (>10000): 400-1000 epoch makul

âš ï¸ 2000 Epoch Riskleri:
â€¢ Overfitting (aÅŸÄ±rÄ± Ã¶ÄŸrenme)
â€¢ Zaman kaybÄ±
â€¢ Kaynak israfÄ±
â€¢ Colab session timeout

âœ… Ã–nerilen YaklaÅŸÄ±m:
1. 200-500 epoch ile baÅŸlayÄ±n
2. Early stopping kullanÄ±n (patience=50)
3. Validation loss'u izleyin
4. Gerekirse epoch sayÄ±sÄ±nÄ± artÄ±rÄ±n

ğŸ›‘ Early Stopping AvantajlarÄ±:
â€¢ Otomatik durdurma
â€¢ En iyi modeli koruma
â€¢ Overfitting Ã¶nleme
â€¢ Zaman tasarrufu

ğŸ“Š Model Boyutuna GÃ¶re Ã–neriler:
â€¢ YOLOv8n (nano): 100-400 epoch
â€¢ YOLOv8s (small): 150-500 epoch  
â€¢ YOLOv8m (medium): 200-600 epoch
â€¢ YOLOv8l (large): 250-800 epoch
â€¢ YOLOv8x (xlarge): 300-1000 epoch

ğŸ¯ SonuÃ§: Early stopping ile baÅŸlayÄ±n, 2000 epoch'u hedef deÄŸil limit olarak gÃ¶rÃ¼n!
    """)
 
 
if __name__ == "__main__":
    # Epoch rehberini yazdÄ±r
    print_epoch_recommendations()
    
    # Ã–rnek optimizasyon
    optimizer = SmartTrainingOptimizer()
    
    # Ã–rnek dataset analizi
    dataset_size = 3000  # KullanÄ±cÄ±nÄ±n dataset boyutu
    
    config = optimizer.get_optimal_training_config(
        dataset_size=dataset_size,
        model_size="yolov8m",
        task_complexity="medium"
    )
    
    print(f"\nğŸ¯ {dataset_size} gÃ¶rÃ¼ntÃ¼lÃ¼ dataset iÃ§in Ã¶neriler:")
    print(f"ğŸ“Š Ã–nerilen epoch: {config['recommended_config']['epochs']}")
    print(f"â±ï¸ Tahmini sÃ¼re: {config['time_estimates']['total_estimated_hours']:.1f} saat")
    print(f"ğŸ›‘ Early stopping patience: {config['recommended_config']['patience']}")
    print(f"ğŸ“ˆ Batch size: {config['recommended_config']['batch_size']}")
    
    print(f"\nğŸ” 2000 Epoch Analizi:")
    analysis = config['epoch_2000_analysis']
    print(f"Karar: {analysis['verdict']}")
    print(f"Sebep: {analysis['reason']}")
    print(f"Risk: {analysis['risk_assessment']}")
    print(f"Ã–neri: {analysis['recommendation']}")
    
    # Optimize edilmiÅŸ script oluÅŸtur
    optimizer.create_training_script(config, "smartfarm_optimized_training.py")
