#!/usr/bin/env python3
# main_multi_dataset.py - HiyerarÅŸik Ã§oklu dataset yÃ¶netimi ve eÄŸitim

import os
import sys
import yaml
from pathlib import Path
import subprocess

# Roboflow API yÃ¶netimi iÃ§in import
try:
    from roboflow_api_helper import get_roboflow_menu_choice, handle_roboflow_action, get_api_key_from_config
except ImportError:
    print("âš ï¸ roboflow_api_helper.py bulunamadÄ±")
    def get_roboflow_menu_choice(): return {}
    def handle_roboflow_action(choice, **kwargs): return False
    def get_api_key_from_config(): return None
import shutil
from datetime import datetime
import json

# Import framework components
from setup_utils import check_gpu, install_required_packages
from hyperparameters import create_hyperparameters_file, load_hyperparameters
from memory_utils import show_memory_usage, clean_memory
from training import train_model, save_to_drive
from training_optimizer import prepare_training_options
try:
    from drive_manager import DriveManager
    _DRIVE_AVAILABLE = True
except Exception:
    _DRIVE_AVAILABLE = False
from model_downloader import download_yolo11_models, download_specific_model_type
from language_manager import get_text, select_language

# Import updated multi-dataset manager
try:
    from multi_dataset_manager import YAMLBasedMultiDatasetManager
except ImportError:
    print("âš ï¸  YAMLBasedMultiDatasetManager not found, trying legacy import...")
    from multi_dataset_manager import MultiDatasetManager as YAMLBasedMultiDatasetManager

# Import hierarchical detection utils
try:
    from hierarchical_detection_utils import HierarchicalDetectionVisualizer
    HIERARCHICAL_DETECTION_AVAILABLE = True
except ImportError:
    print("âš ï¸  HierarchicalDetectionVisualizer not available")
    HIERARCHICAL_DETECTION_AVAILABLE = False

# Import augmentation systems
try:
    from augmentation import TomatoDiseaseAugmentation
    from augmentation import TomatoPestAugmentation
    AUGMENTATION_SYSTEMS_AVAILABLE = True
except ImportError:
    print("âš ï¸  Augmentation systems not available")
    AUGMENTATION_SYSTEMS_AVAILABLE = False

# Ek: Hedefe-tamamlama iÃ§in temel augmentation pipeline (opsiyonel)
try:
    from augmentation_utils import YOLOAugmentationPipeline
    _AUG_PIPE_AVAILABLE = True
except Exception:
    _AUG_PIPE_AVAILABLE = False

# Check if running in Colab
def is_colab():
    """Check if running in Google Colab"""
    try:
        import google.colab
        print("âœ… Google Colab environment detected.")
        return True
    except:
        print("ğŸ’» Running in local environment.")
        return False

def get_tarim_drive_paths():
    """Google Drive'da 'TarÄ±m' taban klasÃ¶rÃ¼nÃ¼ ve alt klasÃ¶rlerini bul/oluÅŸtur.
    DÃ¶nÃ¼ÅŸ: {
      'base': '/content/drive/MyDrive/TarÄ±m',
      'colab_egitim': '/content/drive/MyDrive/Tarim/colab_egitim',
      'yolo11_models': '/content/drive/MyDrive/Tarim/colab_egitim/yolo11_models'
    }
    """
    if not is_colab():
        return None
    # Drive mount kontrolÃ¼
    if not os.path.exists('/content/drive'):
        if not mount_google_drive():
            return None
    mydrive = "/content/drive/MyDrive"
    # TÃ¼rkÃ§e 'TarÄ±m' varsa onu kullan, yoksa 'Tarim' ya da oluÅŸtur
    tarim_candidates = [os.path.join(mydrive, 'TarÄ±m'), os.path.join(mydrive, 'Tarim')]
    base = None
    for c in tarim_candidates:
        if os.path.exists(c):
            base = c
            break
    if base is None:
        # Ã–nce TÃ¼rkÃ§e karakterli klasÃ¶rÃ¼ oluÅŸturmayÄ± dene
        base = tarim_candidates[0]
        try:
            os.makedirs(base, exist_ok=True)
        except Exception:
            base = tarim_candidates[1]
            os.makedirs(base, exist_ok=True)
    colab_egitim = os.path.join(base, 'colab_egitim')
    yolo11_models = os.path.join(base, 'yolo11_models')
    os.makedirs(colab_egitim, exist_ok=True)
    os.makedirs(yolo11_models, exist_ok=True)
    return {'base': base, 'colab_egitim': colab_egitim, 'yolo11_models': yolo11_models}

def get_smartfarm_models_dir():
    """Colab iÃ§in model klasÃ¶rÃ¼nÃ¼ '/content/drive/MyDrive/SmartFarm/colab_learn/yolo11_models' olarak dÃ¶ndÃ¼rÃ¼r ve yoksa oluÅŸturur."""
    if not is_colab():
        return None
    if not os.path.exists('/content/drive'):
        if not mount_google_drive():
            return None
    path = "/content/drive/MyDrive/SmartFarm/colab_learn/yolo11_models"
    os.makedirs(path, exist_ok=True)
    return path

def mount_google_drive():
    """Mount Google Drive"""
    try:
        from google.colab import drive
        drive.mount('/content/drive')
        print("âœ… Google Drive mounted successfully.")
        return True
    except Exception as e:
        print(f"âŒ Error mounting Google Drive: {e}")
        return False

def save_models_to_drive(drive_folder_path, best_file=True, last_file=True):
    """Save best and last model files to Google Drive"""
    if not is_colab():
        print("â„¹ï¸  Bu fonksiyon sadece Google Colab'da Ã§alÄ±ÅŸÄ±r.")
        return False
    
    # Check if Google Drive is mounted
    if not os.path.exists('/content/drive'):
        if not mount_google_drive():
            return False
    
    # Find the most recent training directory
    runs_dir = "runs/train"
    if not os.path.exists(runs_dir):
        print(f"âŒ EÄŸitim dizini bulunamadÄ±: {runs_dir}")
        return False
    
    # Get the latest experiment directory
    exp_dirs = [d for d in os.listdir(runs_dir) if d.startswith('exp')]
    if not exp_dirs:
        print(f"âŒ HiÃ§bir eÄŸitim denemesi bulunamadÄ±: {runs_dir}")
        return False
    
    # Sort to get the latest (exp, exp2, exp3, etc.)
    exp_dirs.sort(key=lambda x: int(x[3:]) if x[3:].isdigit() else 0)
    latest_exp = exp_dirs[-1]
    source_dir = os.path.join(runs_dir, latest_exp, "weights")
    
    if not os.path.exists(source_dir):
        print(f"âŒ AÄŸÄ±rlÄ±k dizini bulunamadÄ±: {source_dir}")
        return False
    
    # Create target directory
    try:
        os.makedirs(drive_folder_path, exist_ok=True)
        print(f"ğŸ“ Hedef dizin oluÅŸturuldu: {drive_folder_path}")
    except Exception as e:
        print(f"âŒ Hedef dizin oluÅŸturulamadÄ±: {e}")
        return False
    
    # Copy files
    copied_files = []
    
    # Copy best.pt
    if best_file:
        best_path = os.path.join(source_dir, "best.pt")
        if os.path.exists(best_path):
            try:
                target_best = os.path.join(drive_folder_path, "best.pt")
                shutil.copy2(best_path, target_best)
                copied_files.append("best.pt")
                print(f"âœ… best.pt kopyalandÄ±: {target_best}")
            except Exception as e:
                print(f"âŒ best.pt kopyalanamadÄ±: {e}")
        else:
            print(f"âš ï¸  best.pt bulunamadÄ±: {best_path}")
    
    # Copy last.pt
    if last_file:
        last_path = os.path.join(source_dir, "last.pt")
        if os.path.exists(last_path):
            try:
                target_last = os.path.join(drive_folder_path, "last.pt")
                shutil.copy2(last_path, target_last)
                copied_files.append("last.pt")
                print(f"âœ… last.pt kopyalandÄ±: {target_last}")
            except Exception as e:
                print(f"âŒ last.pt kopyalanamadÄ±: {e}")
        else:
            print(f"âš ï¸  last.pt bulunamadÄ±: {last_path}")
    
    # Copy additional files from project root
    additional_files = ["merged_dataset.yaml", "unified_class_mapping.json", "analysis_report.json"]
    for file_name in additional_files:
        if os.path.exists(file_name):
            try:
                target_file = os.path.join(drive_folder_path, file_name)
                shutil.copy2(file_name, target_file)
                copied_files.append(file_name)
                print(f"âœ… {file_name} kopyalandÄ±: {target_file}")
            except Exception as e:
                print(f"âŒ {file_name} kopyalanamadÄ±: {e}")
    
    # Copy training results and plots if available
    results_dir = os.path.join(runs_dir, latest_exp)
    result_files = ["results.png", "confusion_matrix.png", "F1_curve.png", "P_curve.png", "R_curve.png"]
    for file_name in result_files:
        result_path = os.path.join(results_dir, file_name)
        if os.path.exists(result_path):
            try:
                target_file = os.path.join(drive_folder_path, file_name)
                shutil.copy2(result_path, target_file)
                copied_files.append(file_name)
                print(f"âœ… {file_name} kopyalandÄ±: {target_file}")
            except Exception as e:
                print(f"âŒ {file_name} kopyalanamadÄ±: {e}")
    
    if copied_files:
        print(f"\nâœ… Google Drive'a kaydedilen dosyalar: {', '.join(copied_files)}")
        print(f"ğŸ“ Kaydetme konumu: {drive_folder_path}")
        print(f"ğŸ—‚ï¸  Toplam kaydedilen dosya: {len(copied_files)}")
        return True
    else:
        print("âŒ Kopyalanacak dosya bulunamadÄ±.")
        return False

# --- YardÄ±mcÄ±: master sÄ±nÄ±f isimlerini yÃ¼kleyip class_ids.json yaz ---
def _load_names_from_yaml(yaml_path: str):
    try:
        if os.path.exists(yaml_path):
            with open(yaml_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f) or {}
            names = []
            if isinstance(data.get('names'), list):
                names = data['names']
            elif isinstance(data.get('names'), dict):
                names = [v for k, v in sorted(((int(k), v) for k, v in data['names'].items()), key=lambda x: x[0])]
            elif isinstance(data.get('classes'), list):
                names = data['classes']
            return [str(x) for x in names] if names else None
    except Exception:
        return None
    return None

def _write_class_ids_json(configs_dir: str) -> bool:
    try:
        # Ã–ncelik master_data.yaml, yoksa merged_dataset.yaml
        names = _load_names_from_yaml('master_data.yaml') or _load_names_from_yaml('merged_dataset.yaml')
        if not names:
            return False
        payload = {
            'generated_at': datetime.now().isoformat(),
            'names': names,
            'id_to_name': [{'id': i, 'name': n} for i, n in enumerate(names)],
        }
        os.makedirs(configs_dir, exist_ok=True)
        out_path = os.path.join(configs_dir, 'class_ids.json')
        with open(out_path, 'w', encoding='utf-8') as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)
        print(f"âœ… class_ids.json yazÄ±ldÄ±: {out_path}")
        return True
    except Exception as e:
        print(f"âš ï¸ class_ids.json yazÄ±lamadÄ±: {e}")
        return False

def download_models_menu():
    """Interactive menu for downloading YOLO11 models"""
    print(f"\n{get_text('model_download_title')}")
    
    if is_colab():
        default_dir = get_smartfarm_models_dir() or "/content/colab_learn/yolo11_models"
    else:
        default_dir = "yolo11_models"
    save_dir = input(get_text('save_directory', default=default_dir)) or default_dir
    
    print(f"\n{get_text('download_options')}")
    print(get_text('single_model'))
    print(get_text('all_detection'))
    print(get_text('all_models'))
    
    choice = input(f"\n{get_text('your_choice')}") or "2"
    
    if choice == "1":
        print(f"\n{get_text('select_model_type')}")
        print(get_text('detection_default'))
        print(get_text('segmentation'))
        print(get_text('classification'))
        print(get_text('pose'))
        print(get_text('obb'))
        
        model_type_map = {
            "1": "detection",
            "2": "segmentation", 
            "3": "classification",
            "4": "pose",
            "5": "obb"
        }
        
        model_type_choice = input(get_text('enter_choice_1_5')) or "1"
        model_type = model_type_map.get(model_type_choice, "detection")
        
        print(f"\n{get_text('select_model_size')}")
        print(get_text('small'))
        print(get_text('medium_default'))
        print(get_text('large'))
        print(get_text('extra_large'))
        
        size_map = {
            "1": "s",
            "2": "m",
            "3": "l",
            "4": "x"
        }
        
        size_choice = input(get_text('enter_choice_1_4')) or "2"
        size = size_map.get(size_choice, "m")
        
        model_path = download_specific_model_type(model_type, size, save_dir)
        if model_path:
            print(f"\nâœ… Model baÅŸarÄ±yla indirildi: {model_path}")
    
    elif choice == "2":
        detection_models = ["yolo11s.pt", "yolo11m.pt", "yolo11l.pt", "yolo11x.pt"]
        downloaded = download_yolo11_models(save_dir, detection_models)
        print(f"\nâœ… {len(downloaded)} tespit modeli indirildi: {save_dir}")
    
    elif choice == "3":
        downloaded = download_yolo11_models(save_dir)
        print(f"\nâœ… {len(downloaded)} model indirildi: {save_dir}")
    
    else:
        print("\nâŒ GeÃ§ersiz seÃ§im. HiÃ§bir model indirilmedi.")
        return None
    
    return save_dir

def hierarchical_dataset_setup():
    """Setup for hierarchical multi-dataset training"""
    print("\n===== HiyerarÅŸik Ã‡oklu Veri Seti Kurulumu =====")
    
    # Initialize the YAML-based dataset manager
    config_file = input("KonfigÃ¼rasyon dosyasÄ± yolu (varsayÄ±lan: config_datasets.yaml): ") or "config_datasets.yaml"
    
    if not os.path.exists(config_file):
        print(f"âŒ KonfigÃ¼rasyon dosyasÄ± bulunamadÄ±: {config_file}")
        print("LÃ¼tfen config_datasets.yaml dosyasÄ±nÄ±n mevcut dizinde olduÄŸundan emin olun")
        return None
    
    manager = YAMLBasedMultiDatasetManager(config_file=config_file)
    
    # Opsiyonel: Roboflow API key giriÅŸi (boÅŸ bÄ±rakÄ±labilir)
    try:
        print("\nğŸ”‘ Roboflow API (opsiyonel)")
        entered_key = input("API key girin (boÅŸ geÃ§ebilirsiniz): ").strip()
        if entered_key:
            manager.api_key = entered_key
            # API key girildiyse, kullanÄ±cÄ±ya split ayarÄ±nÄ± da soralÄ±m
            split_cfg = get_dataset_split_config(entered_key)
            if split_cfg:
                manager.split_config = split_cfg
        else:
            # BoÅŸsa, varsa config'den otomatik kullanÄ±lacak (indirme sÄ±rasÄ±nda fallback var)
            manager.api_key = None
            manager.split_config = None
    except Exception:
        # Sessiz geÃ§
        pass

    # Show system information
    print(f"\nğŸ“Š Sistem Bilgileri:")
    print(f"âœ… KonfigÃ¼rasyon yÃ¼klendi: {config_file}")
    print(f"ğŸ“ Mevcut gruplar: {len(manager.get_available_dataset_groups())}")
    
    # Interactive dataset selection
    selected_group = manager.interactive_dataset_selection()
    
    if not selected_group:
        print("âŒ HiÃ§bir veri seti grubu seÃ§ilmedi")
        return None
    
    # Get recommendations
    recommendations = manager.get_training_recommendations(selected_group)
    
    print(f"\nğŸ¯ '{selected_group}' iÃ§in EÄŸitim Ã–nerileri:")
    for key, value in recommendations.items():
        if isinstance(value, list):
            print(f"  {key}:")
            for item in value:
                print(f"    â€¢ {item}")
        else:
            print(f"  {key}: {value}")
    
    # Get global settings
    settings = manager.get_global_settings()
    
    # Ask for target count per class (global default + opsiyonel sÄ±nÄ±f bazlÄ± hedefler)
    default_target = settings.get('default_target_count_per_class', 5000)
    while True:
        try:
            target_count = int(input(f"\nSÄ±nÄ±f baÅŸÄ±na hedef Ã¶rnek sayÄ±sÄ± (varsayÄ±lan: {default_target}): ") or str(default_target))
            if target_count > 0:
                break
            print("âŒ LÃ¼tfen pozitif bir sayÄ± girin.")
        except ValueError:
            print("âŒ LÃ¼tfen geÃ§erli bir sayÄ± girin.")

    # Opsiyonel: KullanÄ±cÄ± sÄ±nÄ±f bazÄ±nda Ã¶zel hedef sayÄ±larÄ± girmek isterse
    per_class_targets = None
    customize = (input("\nSÄ±nÄ±f bazÄ±nda hedef sayÄ±larÄ± Ã¶zelleÅŸtirmek ister misiniz? (e/h, varsayÄ±lan: h): ") or "h").lower()
    if customize.startswith('e'):
        per_class_targets = {}
        print("\nSÄ±nÄ±f bazlÄ± hedefler (boÅŸ bÄ±rakÄ±lÄ±rsa genel varsayÄ±lan kullanÄ±lacak):")
        for cls in manager.hierarchical_classes.keys():
            try:
                val = input(f"  â€¢ {cls} iÃ§in hedef (varsayÄ±lan {target_count}): ") or str(target_count)
                if val.strip() == "":
                    continue
                n = int(val)
                if n > 0:
                    per_class_targets[cls] = n
            except Exception:
                pass
    
    # Output directory
    default_output = "datasets/hierarchical_merged"
    output_dir = input(f"\nBirleÅŸtirilmiÅŸ veri seti dizini (varsayÄ±lan: {default_output}): ") or default_output

    return {
        'manager': manager,
        'selected_group': selected_group,
        'target_count': target_count,
        'per_class_targets': per_class_targets,
        'output_dir': output_dir,
        'recommendations': recommendations,
        'settings': settings
    }

def process_hierarchical_datasets(dataset_config):
    """Process hierarchical multi-datasets"""
    print("\n===== HiyerarÅŸik Ã‡oklu Veri Setleri Ä°ÅŸleniyor =====")
    
    manager = dataset_config['manager']
    target_count = dataset_config['target_count']
    
    try:
        # 1. Download datasets
        print("\n1ï¸âƒ£ Veri setleri indiriliyor...")
        download_success = manager.download_all_datasets()
        
        if not download_success:
            print("âŒ Veri seti indirme baÅŸarÄ±sÄ±z!")
            return False

        # Etiket Yeniden EÅŸleme Modu â€” Ä°NDÄ°RME SONRASI taÅŸÄ±ndÄ±
        print("\nEtiket Yeniden EÅŸleme Modu:")
        print("1) Merge aÅŸamasÄ±nda alt-sÄ±nÄ±f etiketleri KORUNMAZ; tÃ¼m kutular ANA sÄ±nÄ±fa toplanÄ±r (varsayÄ±lan)")
        print("2) Merge aÅŸamasÄ±nda alt-sÄ±nÄ±f etiketleri KORUNUR; tÃ¼m kutular ana sÄ±nÄ±fa toplanmaz")
        while True:
            label_mode_choice = (input("SeÃ§enek [1-2] (varsayÄ±lan: 1): ") or "1").strip()
            if label_mode_choice in ["1", "2"]:
                break
            print("âŒ LÃ¼tfen 1 veya 2 giriniz.")
        dataset_config['label_mode'] = "collapse_to_main" if label_mode_choice == "1" else "preserve_subclasses"

        # 2. Create unified class mapping
        print("\n2ï¸âƒ£ HiyerarÅŸik sÄ±nÄ±f haritalamasÄ± oluÅŸturuluyor...")
        classes_created = manager.create_unified_class_mapping()
        
        if classes_created == 0:
            print("âŒ HiÃ§bir sÄ±nÄ±f haritalandÄ±rÄ±lamadÄ±!")
            return False
        
        print(f"âœ… {classes_created} ana sÄ±nÄ±f oluÅŸturuldu")

        # 3. Label mode yÃ¶nlendirmesi
        label_mode = dataset_config.get('label_mode') or dataset_config.get('setup', {}).get('label_mode')
        if label_mode == 'preserve_subclasses':
            print("\nâš™ï¸ SeÃ§enek 2: Alt-sÄ±nÄ±f etiketleri KORUNACAK. Global remap otomatik baÅŸlatÄ±lÄ±yor...")
            try:
                cmd = [
                    sys.executable,
                    str(Path("tools") / "remap_from_all_yaml.py"),
                    "--root", "datasets",
                    "--force-backup",
                ]
                print(f"[APPLY] Remap komutu: {' '.join(cmd)}")
                proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                if proc.stdout:
                    print(proc.stdout)
                if proc.returncode != 0:
                    print(f"[UYARI] Remap sÄ±rasÄ±nda hata oluÅŸtu (kod={proc.returncode}). Devam ediliyor...\n{proc.stderr}")
                else:
                    print("âœ… Etiket remap iÅŸlemi tamamlandÄ±.")
            except Exception as e:
                print(f"[UYARI] Remap Ã§aÄŸrÄ±sÄ± baÅŸarÄ±sÄ±z: {e}. Ä°ÅŸleme devam edilecek.")

        # 4. Merge step: only for option 1 (collapse_to_main)
        if label_mode == 'preserve_subclasses':
            # Kopyalama yapmadan Ã§oklu kaynak YAML Ã¼ret
            try:
                def _resolve_split_image_dirs(local_root: str, split: str):
                    """Split iÃ§in olasÄ± gÃ¶rÃ¼ntÃ¼ klasÃ¶rlerini tespit et."""
                    cand = []
                    data_yaml_path = os.path.join(local_root, 'data.yaml')
                    if os.path.exists(data_yaml_path):
                        try:
                            with open(data_yaml_path, 'r', encoding='utf-8') as f:
                                dcfg = yaml.safe_load(f) or {}
                            base_path = dcfg.get('path') or '.'
                            entry = dcfg.get('train' if split == 'train' else ('val' if 'val' in split else split))
                            # val/valid eÅŸ anlamlÄ±
                            if not entry and split == 'val':
                                entry = dcfg.get('valid')
                            if entry:
                                def _join(root_dir, p):
                                    return p if os.path.isabs(p) else os.path.normpath(os.path.join(root_dir, p))
                                full = _join(local_root, _join('' if base_path == '.' else base_path, entry))
                                cand.append(full)
                                # images -> ensure it's images dir
                                if os.path.basename(full) != 'images' and os.path.isdir(full):
                                    # kullanÄ±cÄ± doÄŸrudan images klasÃ¶rÃ¼nÃ¼ deÄŸil Ã¼st klasÃ¶rÃ¼ vermiÅŸ olabilir
                                    cand.append(os.path.join(full, 'images'))
                        except Exception:
                            pass
                    # YaygÄ±n dizinler
                    synonyms = [split]
                    if split == 'val':
                        synonyms.append('valid')
                    if split == 'valid':
                        synonyms.append('val')
                    for s in synonyms:
                        cand.extend([
                            os.path.join(local_root, 'images', s),
                            os.path.join(local_root, s, 'images'),
                        ])
                    # Var olanlarÄ± filtrele
                    uniq = []
                    seen = set()
                    for p in cand:
                        if p and os.path.isdir(p) and p not in seen:
                            uniq.append(os.path.abspath(p))
                            seen.add(p)
                    return uniq

                # Label klasÃ¶rlerini bulmak iÃ§in yardÄ±mcÄ±
                def _resolve_split_label_dirs(local_root: str, split: str):
                    cand = []
                    data_yaml_path = os.path.join(local_root, 'data.yaml')
                    if os.path.exists(data_yaml_path):
                        try:
                            with open(data_yaml_path, 'r', encoding='utf-8') as f:
                                dcfg = yaml.safe_load(f) or {}
                            base_path = dcfg.get('path') or '.'
                            entry = dcfg.get('train' if split == 'train' else ('val' if 'val' in split else split))
                            if not entry and split == 'val':
                                entry = dcfg.get('valid')
                            if entry:
                                def _join(root_dir, p):
                                    return p if os.path.isabs(p) else os.path.normpath(os.path.join(root_dir, p))
                                full = _join(local_root, _join('' if base_path == '.' else base_path, entry))
                                if os.path.isdir(full):
                                    # images/... kalÄ±bÄ±ndan labels/... tahmini
                                    cand.append(full.replace(os.sep + 'images' + os.sep, os.sep + 'labels' + os.sep))
                                    cand.append(os.path.join(os.path.dirname(full), 'labels'))
                        except Exception:
                            pass
                    synonyms = [split]
                    if split == 'val':
                        synonyms.append('valid')
                    if split == 'valid':
                        synonyms.append('val')
                    for s in synonyms:
                        cand.extend([
                            os.path.join(local_root, 'labels', s),
                            os.path.join(local_root, s, 'labels'),
                        ])
                    uniq = []
                    seen = set()
                    for p in cand:
                        if p and os.path.isdir(p) and p not in seen:
                            uniq.append(os.path.abspath(p))
                            seen.add(p)
                    return uniq

                # TÃ¼m veri setlerinden train/val klasÃ¶rlerini topla
                train_dirs = []
                val_dirs = []
                for ds in manager.datasets:
                    root = ds.get('local_path')
                    if not root or not os.path.isdir(root):
                        continue
                    train_dirs.extend(_resolve_split_image_dirs(root, 'train'))
                    # val bulunamazsa valid dene
                    vds = _resolve_split_image_dirs(root, 'val')
                    if not vds:
                        vds = _resolve_split_image_dirs(root, 'valid')
                    # HiÃ§ val yoksa train'i yedek olarak kullanma â€” eÄŸitim iÃ§in val gerekli, ama yoksa boÅŸ bÄ±rakma yerine train'den kÃ¼Ã§Ã¼k bir altkÃ¼me oluÅŸturmanÄ±zÄ± Ã¶neririz.
                    val_dirs.extend(vds)

                # names yÃ¼kle
                def _load_master_names():
                    for p in ['config/master_data.yaml', 'master_data.yaml']:
                        if os.path.exists(p):
                            try:
                                with open(p, 'r', encoding='utf-8') as f:
                                    data = yaml.safe_load(f) or {}
                                names = data.get('names') or data.get('classes')
                                if isinstance(names, list) and names:
                                    return [str(x) for x in names]
                            except Exception:
                                pass
                    # Fallback: indirilen veri setlerinden birlik
                    union = []
                    seen = set()
                    for ds in manager.datasets:
                        for c in ds.get('classes', []) or []:
                            if c not in seen:
                                union.append(c)
                                seen.add(c)
                    return union

                # Ä°simleri Ã¶ncelikle config/class_ids.json'dan al
                names = None
                try:
                    cid_path = os.path.join('config', 'class_ids.json')
                    if os.path.exists(cid_path):
                        with open(cid_path, 'r', encoding='utf-8') as f:
                            cid = json.load(f)
                        if isinstance(cid.get('names'), list) and cid['names']:
                            names = [str(x) for x in cid['names']]
                except Exception:
                    pass
                if not names:
                    names = _load_master_names()
                if not train_dirs:
                    print("âŒ HiÃ§bir train dizini bulunamadÄ±. YAML oluÅŸturulamadÄ±.")
                    return False
                if not val_dirs:
                    print("âš ï¸ Val/valid dizini bulunamadÄ±. Train dizinleri kullanÄ±lacak (deÄŸerlendirme iÃ§in Ã¶nerilmez).")
                    val_dirs = train_dirs[:]

                dataset_yaml = {
                    'path': '.',
                    'train': train_dirs,
                    'val': val_dirs,
                    'nc': len(names),
                    'names': names,
                }
                os.makedirs('config', exist_ok=True)
                merged_yaml_path = os.path.join('config', 'merged_class_dataset.yaml')
                with open(merged_yaml_path, 'w', encoding='utf-8') as f:
                    yaml.dump(dataset_yaml, f, sort_keys=False, allow_unicode=True)
                print("ğŸ“„ config/merged_class_dataset.yaml oluÅŸturuldu (kopyasÄ±z Ã§oklu kaynak).")
                print(f"  â€¢ train kaynak sayÄ±sÄ±: {len(train_dirs)}")
                print(f"  â€¢ val kaynak sayÄ±sÄ±: {len(val_dirs)}")
            except Exception as e:
                print(f"âŒ merged_dataset.yaml oluÅŸturulurken hata: {e}")
                return False
            # Merge kopyalama adÄ±mÄ± atlanÄ±r fakat eÄŸitim iÃ§in YAML hazÄ±r.
            print("\nâ„¹ï¸ SeÃ§enek 2 iÃ§in kopyalama yapmadan eÄŸitim YAML hazÄ±rlandÄ± (config/merged_class_dataset.yaml).")

            # Opsiyonel: Hedefe TAMAMLAMA augmentation (yalnÄ±zca eksik kadar Ã¼ret)
            try:
                if _AUG_PIPE_AVAILABLE:
                    # e/h girdisi iÃ§in doÄŸrulama dÃ¶ngÃ¼sÃ¼
                    while True:
                        raw = input("\nSÄ±nÄ±f baÅŸÄ±na hedefe TAMAMLAMA iÃ§in augmentation uygulansÄ±n mÄ±? (e/h, varsayÄ±lan: h): ")
                        resp = (raw or 'h').strip().lower()
                        if resp in ('e', 'h'):
                            break
                        print("GeÃ§ersiz giriÅŸ. LÃ¼tfen 'e' veya 'h' girin.")
                    if resp == 'e':
                        # Hedef belirleme: sayÄ±sal doÄŸrulama dÃ¶ngÃ¼sÃ¼
                        target_default = dataset_config.get('target_count')
                        try:
                            target_default = int(target_default)
                        except Exception:
                            target_default = 2000
                        while True:
                            raw_t = input(f"Hedef sÄ±nÄ±f baÅŸÄ±na Ã¶rnek (boÅŸ bÄ±rak: {target_default}): ")
                            if not raw_t or not raw_t.strip():
                                target_numeric = target_default
                                break
                            try:
                                target_numeric = int(raw_t.strip())
                                if target_numeric <= 0:
                                    print("GeÃ§ersiz deÄŸer. Pozitif bir tam sayÄ± girin.")
                                    continue
                                break
                            except ValueError:
                                print("GeÃ§ersiz deÄŸer. LÃ¼tfen bir tam sayÄ± girin.")

                        # Train label ve image dosyalarÄ±nÄ± topla
                        image_dirs = train_dirs[:]
                        label_dirs = []
                        for ds in manager.datasets:
                            root = ds.get('local_path')
                            if not root or not os.path.isdir(root):
                                continue
                            label_dirs.extend(_resolve_split_label_dirs(root, 'train'))

                        def _iter_label_files(dirs):
                            for d in dirs:
                                for root_dir, _, files in os.walk(d):
                                    for fn in files:
                                        if fn.lower().endswith('.txt'):
                                            yield os.path.join(root_dir, fn)

                        def _match_image_for_label(label_path, img_dirs):
                            base = os.path.splitext(os.path.basename(label_path))[0]
                            for d in img_dirs:
                                for ext in ['.jpg', '.jpeg', '.png']:
                                    cand = os.path.join(d, base + ext)
                                    if os.path.exists(cand):
                                        return cand
                            return None

                        all_label_paths = []
                        all_image_paths = []
                        for lp in _iter_label_files(label_dirs):
                            ip = _match_image_for_label(lp, image_dirs)
                            if ip:
                                all_label_paths.append(lp)
                                all_image_paths.append(ip)

                        if not all_label_paths:
                            print("âš ï¸  Augmentation iÃ§in train etiket dosyasÄ± bulunamadÄ±. AdÄ±m atlandÄ±.")
                        else:
                            out_dir = os.path.join('datasets', 'balanced_aug')
                            os.makedirs(out_dir, exist_ok=True)
                            print(f"\nâš™ï¸  Hedefe-tamamlama baÅŸlÄ±yor. Hedef: {target_numeric}  Ã‡Ä±kÄ±ÅŸ: {out_dir}")
                            pipe = YOLOAugmentationPipeline(image_size=dataset_config.get('settings', {}).get('default_image_size', 640))
                            
                            # Resolve VAL/TEST image/label directories from all datasets
                            val_img_dirs = []
                            val_lbl_dirs = []
                            test_img_dirs = []
                            test_lbl_dirs = []
                            for ds in manager.datasets:
                                root = ds.get('local_path')
                                if not root or not os.path.isdir(root):
                                    continue
                                # val/valid images
                                vids = _resolve_split_image_dirs(root, 'val')
                                if not vids:
                                    vids = _resolve_split_image_dirs(root, 'valid')
                                val_img_dirs.extend(vids)
                                # val/valid labels
                                vlds = _resolve_split_label_dirs(root, 'val')
                                if not vlds:
                                    vlds = _resolve_split_label_dirs(root, 'valid')
                                val_lbl_dirs.extend(vlds)
                                # test images/labels
                                test_img_dirs.extend(_resolve_split_image_dirs(root, 'test'))
                                test_lbl_dirs.extend(_resolve_split_label_dirs(root, 'test'))

                            # Decide automatically (no prompt): copy val/test based on config defaults
                            default_copy_val_test = None
                            try:
                                default_copy_val_test = dataset_config.get('settings', {}).get('copy_val_test_default')
                                if default_copy_val_test is not None:
                                    default_copy_val_test = bool(default_copy_val_test)
                            except Exception:
                                default_copy_val_test = None
                            if default_copy_val_test is None:
                                try:
                                    with open(os.path.join('config', 'master_data.yaml'), 'r', encoding='utf-8') as f:
                                        _md = yaml.safe_load(f) or {}
                                    default_copy_val_test = bool(_md.get('copy_val_test_default', True))
                                except Exception:
                                    default_copy_val_test = True
                            copy_val_test = bool(default_copy_val_test)

                            pipe.augment_dataset_batch(
                                all_image_paths,
                                all_label_paths,
                                out_dir,
                                target_numeric,
                                copy_val_test=copy_val_test,
                                val_image_dirs=val_img_dirs,
                                val_label_dirs=val_lbl_dirs,
                                test_image_dirs=test_img_dirs,
                                test_label_dirs=test_lbl_dirs,
                            )

                            # Create config/augmented_train.yaml to reference augmented train and original or copied val/test
                            try:
                                # Determine YAML val/test paths
                                if copy_val_test:
                                    yaml_val = os.path.join(out_dir, 'val', 'images')
                                    yaml_test = os.path.join(out_dir, 'test', 'images') if test_img_dirs or test_lbl_dirs else None
                                else:
                                    # Use original directories (list is supported)
                                    yaml_val = val_img_dirs if val_img_dirs else None
                                    yaml_test = test_img_dirs if test_img_dirs else None

                                if not names:
                                    # Try to reload names from config/class_ids.json as fallback
                                    try:
                                        with open(os.path.join('config', 'class_ids.json'), 'r', encoding='utf-8') as f:
                                            cid = json.load(f)
                                        if isinstance(cid.get('names'), list) and cid['names']:
                                            local_names = [str(x) for x in cid['names']]
                                        else:
                                            local_names = []
                                    except Exception:
                                        local_names = []
                                else:
                                    local_names = names

                                yaml_payload = {
                                    'path': '.',
                                    'train': os.path.join(out_dir, 'train', 'images'),
                                    'val': yaml_val if yaml_val is not None else [],
                                    'test': yaml_test if yaml_test is not None else [],
                                    'names': local_names,
                                }
                                os.makedirs('config', exist_ok=True)
                                aug_yaml_path = os.path.join('config', 'augmented_train.yaml')
                                with open(aug_yaml_path, 'w', encoding='utf-8') as f:
                                    yaml.dump(yaml_payload, f, sort_keys=False, allow_unicode=True)
                                print(f"\nğŸ“„ EÄŸitim YAML yazÄ±ldÄ±: {aug_yaml_path}")
                                print(f"  â€¢ train: {yaml_payload['train']}")
                                print(f"  â€¢ val:   {yaml_payload['val'] if yaml_payload['val'] else 'â€”'}")
                                print(f"  â€¢ test:  {yaml_payload['test'] if yaml_payload['test'] else 'â€”'}")

                                # Optional: Ask user to enable time-based Drive copy for training (default: disabled)
                                try:
                                    cfg_path = 'config_datasets.yaml'
                                    def_minutes = 30
                                    if os.path.exists(cfg_path):
                                        with open(cfg_path, 'r', encoding='utf-8') as _cf:
                                            _cfg = yaml.safe_load(_cf) or {}
                                        gs = _cfg.get('global_settings', {}) if isinstance(_cfg, dict) else {}
                                        def_minutes = int(gs.get('time_based_copy_interval_minutes', 30))
                                    yn = (input("\nSÃ¼reye baÄŸlÄ± Drive kopyalama aÃ§Ä±lsÄ±n mÄ±? (e/h, varsayÄ±lan: h): ") or 'h').strip().lower()
                                    use_time_based = yn.startswith('e')
                                    minutes = def_minutes
                                    if use_time_based:
                                        min_in = input(f"Kopyalama aralÄ±ÄŸÄ± (dakika, varsayÄ±lan: {def_minutes}): ").strip()
                                        minutes = int(min_in) if min_in else def_minutes
                                    # Persist selection into config_datasets.yaml
                                    try:
                                        _cfg = {}
                                        if os.path.exists(cfg_path):
                                            with open(cfg_path, 'r', encoding='utf-8') as _cf:
                                                _cfg = yaml.safe_load(_cf) or {}
                                        if not isinstance(_cfg, dict):
                                            _cfg = {}
                                        _gs = _cfg.get('global_settings')
                                        if not isinstance(_gs, dict):
                                            _gs = {}
                                        _gs['use_time_based_copy_default'] = bool(use_time_based)
                                        _gs['time_based_copy_interval_minutes'] = int(minutes)
                                        _cfg['global_settings'] = _gs
                                        with open(cfg_path, 'w', encoding='utf-8') as _cf:
                                            yaml.dump(_cfg, _cf, sort_keys=False, allow_unicode=True)
                                        state_txt = 'AÃ‡IK' if use_time_based else 'KAPALI'
                                        print(f"âœ… SÃ¼reye baÄŸlÄ± kopyalama ayarÄ± gÃ¼ncellendi: {state_txt}, {minutes} dk")
                                        # AyrÄ±ca kullanÄ±cÄ± seÃ§imini configs/ klasÃ¶rÃ¼ne de kaydet
                                        try:
                                            os.makedirs('configs', exist_ok=True)
                                            from datetime import datetime as _dt
                                            ts = _dt.now().strftime('%Y%m%d_%H%M%S')
                                            snapshot = {
                                                'timestamp': ts,
                                                'use_time_based_copy_default': bool(use_time_based),
                                                'time_based_copy_interval_minutes': int(minutes),
                                                'augmented_train_yaml': aug_yaml_path,
                                            }
                                            snap_path = os.path.join('configs', f'time_copy_selection_{ts}.yaml')
                                            with open(snap_path, 'w', encoding='utf-8') as _sf:
                                                yaml.dump(snapshot, _sf, sort_keys=False, allow_unicode=True)
                                            print(f"ğŸ“ SeÃ§im kopyasÄ± kaydedildi: {snap_path}")
                                        except Exception as _snap_e:
                                            print(f"âš ï¸ SeÃ§im kopyasÄ± kaydedilemedi: {_snap_e}")
                                    except Exception as _werr:
                                        print(f"âš ï¸ SÃ¼reye baÄŸlÄ± kopyalama ayarÄ± kaydedilemedi: {_werr}")
                                except Exception as _tbc_err:
                                    print(f"âš ï¸ SÃ¼reye baÄŸlÄ± kopyalama ayarÄ± sorulamadÄ±: {_tbc_err}")
                            except Exception as _yaml_err:
                                print(f"âš ï¸ augmented_train.yaml oluÅŸturulamadÄ±: {_yaml_err}")
                            print("âœ… Hedefe-tamamlama augmentation tamamlandÄ±.")
                else:
                    print("âš ï¸  YOLOAugmentationPipeline mevcut deÄŸil. Hedefe-tamamlama atlandÄ±.")
            except Exception as e:
                print(f"[UYARI] Hedefe-tamamlama sÄ±rasÄ±nda hata: {e}")

            return True
        
        print("\n3ï¸âƒ£ Veri setleri hiyerarÅŸik yapÄ±yla birleÅŸtiriliyor...")
        # Fonksiyona 'setup' dict'i geÃ§irildiÄŸi iÃ§in doÄŸrudan buradan oku
        pct = dataset_config.get('per_class_targets')
        target_arg = pct if pct else target_count
        merged_counts = manager.merge_datasets(target_count_per_class=target_arg)
        
        if not merged_counts:
            print("âŒ Veri seti birleÅŸtirme baÅŸarÄ±sÄ±z!")
            return False
        
        print(f"\nâœ… HiyerarÅŸik Ã§oklu veri seti iÅŸleme tamamlandÄ±!")
        print(f"ğŸ“ BirleÅŸtirilmiÅŸ veri seti: {manager.output_dir}")
        print(f"ğŸ“„ YAML dosyasÄ±: merged_dataset.yaml")
        print(f"ğŸ·ï¸  SÄ±nÄ±f haritasÄ±: unified_class_mapping.json")
        
        # Display final statistics
        total_samples = sum(merged_counts.values())
        print(f"\nğŸ“Š Son Veri Seti Ä°statistikleri:")
        print(f"   Toplam Ã¶rnek: {total_samples:,}")
        print(f"   Ana sÄ±nÄ±flar: {len(merged_counts)}")
        print(f"   SÄ±nÄ±f baÅŸÄ±na Ã¶rnek: {total_samples // len(merged_counts):,} (ortalama)")
        
        for class_name, count in merged_counts.items():
            print(f"   {class_name}: {count:,}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ HiyerarÅŸik veri seti iÅŸleme sÄ±rasÄ±nda hata: {e}")
        import traceback
        traceback.print_exc()
        return False

def interactive_training_setup():
    """Interactive training parameter setup for hierarchical model"""
    print("\n===== HiyerarÅŸik Model EÄŸitim Kurulumu =====")
    
    # Dataset type selection
    print("\nVeri seti konfigÃ¼rasyonu:")
    print("1) HiyerarÅŸik Ã§oklu veri seti (Ã–nerilen)")
    print("2) Tek Roboflow veri seti (Eski)")
    
    while True:
        dataset_choice = input("\nSeÃ§enek [1-2] (varsayÄ±lan: 1): ") or "1"
        if dataset_choice in ["1", "2"]:
            break
        print("âŒ LÃ¼tfen 1 veya 2 seÃ§in.")
    
    dataset_config = {}
    
    if dataset_choice == "1":
        # Hierarchical multi-dataset
        dataset_setup = hierarchical_dataset_setup()
        if not dataset_setup:
            return None
        
        dataset_config = {
            'type': 'hierarchical_multi',
            'setup': dataset_setup,
            'data_yaml': 'merged_dataset.yaml'
        }
    else:
        # Single dataset (legacy) - Roboflow API yÃ¶netimi ile
        roboflow_url = input("\nRoboflow URL (varsayÄ±lan: boÅŸ): ").strip() or ""
        if not roboflow_url:
            print("âŒ URL saÄŸlanmadÄ±")
            return None
        
        # Roboflow API key yÃ¶netimi
        api_result = handle_roboflow_api_management(roboflow_url)
        
        dataset_config = {
            'type': 'single',
            'url': roboflow_url,
            'api_key': api_result['api_key'],
            'split_config': api_result['split_config'],
            'data_yaml': 'dataset.yaml'
        }
    
    # Project category
    print("\nProje kategorisi:")
    print("1) HiyerarÅŸik TarÄ±msal AI (Ã–nerilen)")
    print("2) HastalÄ±k Tespiti")
    print("3) ZararlÄ± Tespiti")
    print("4) Karma TarÄ±msal")
    print("5) Ã–zel")
    
    while True:
        category_choice = input("\nKategori seÃ§in [1-5] (varsayÄ±lan: 1): ") or "1"
        category_options = {
            "1": "hierarchical_agricultural",
            "2": "diseases",
            "3": "pests", 
            "4": "mixed",
            "5": "custom"
        }
        
        if category_choice in category_options:
            category = category_options[category_choice]
            if category == "custom":
                category = input("Ã–zel kategori adÄ± girin: ").strip() or "custom"
            break
        print("âŒ LÃ¼tfen 1-5 arasÄ± seÃ§in.")
    
    # Get training recommendations
    if dataset_config['type'] == 'hierarchical_multi':
        recommendations = dataset_config['setup']['recommendations']
        recommended_model = recommendations.get('model', 'yolo11l.pt')
        recommended_batch = recommendations.get('batch_size', 8)
        recommended_size = recommendations.get('image_size', 640)
        estimated_time = recommendations.get('estimated_time', 'Unknown')
        
        print(f"\nğŸ¯ HiyerarÅŸik model iÃ§in Ã¶neriler:")
        print(f"   Model: {recommended_model}")
        print(f"   Batch boyutu: {recommended_batch}")
        print(f"   GÃ¶rÃ¼ntÃ¼ boyutu: {recommended_size}")
        print(f"   Tahmini sÃ¼re: {estimated_time}")
        
        # Show special notes if available
        special_notes = recommendations.get('special_notes', [])
        if special_notes:
            print(f"   Ã–zel hususlar:")
            for note in special_notes:
                print(f"     â€¢ {note}")
    
    # Google Drive save settings (daha erken sorulsun)
    drive_save_path = None
    if is_colab():
        print("\nGoogle Drive kaydetme ayarlarÄ±:")
        save_to_drive_opt = input("EÄŸitim sonuÃ§larÄ±nÄ± Google Drive'a kaydet? (e/h, varsayÄ±lan: e): ").lower() or "e"
        
        if save_to_drive_opt.startswith("e"):
            default_drive_path = get_smartfarm_models_dir() or "/content/drive/MyDrive/SmartFarm/colab_learn/yolo11_models"
            base_input = input(f"Kaydetme dizini (varsayÄ±lan: {default_drive_path}): ") or default_drive_path

            # Varsa mevcut timestamp klasÃ¶rÃ¼nÃ¼ KULLAN, yoksa oluÅŸtur
            timestamp_dir = None
            try:
                # 1) DriveManager'da aktif timestamp var mÄ±?
                if _DRIVE_AVAILABLE:
                    dm_probe = DriveManager()
                    if dm_probe.authenticate():
                        try:
                            if hasattr(dm_probe, 'load_drive_config'):
                                dm_probe.load_drive_config()
                        except Exception:
                            pass
                        ts_existing = dm_probe.get_timestamp_dir()
                        if ts_existing and os.path.basename(os.path.dirname(ts_existing)) == 'yolo11_models':
                            timestamp_dir = ts_existing
                # 2) Base klasÃ¶rde mevcut timestamp dizinlerini tara ve ILK OLUÅANINI al (ilk timestamp kuralÄ±)
                if not timestamp_dir and os.path.isdir(base_input):
                    candidates = [
                        os.path.join(base_input, d)
                        for d in os.listdir(base_input)
                        if os.path.isdir(os.path.join(base_input, d)) and TIMESTAMP_PATTERN.match(d)
                    ]
                    if candidates:
                        # mtime'a gÃ¶re artan sÄ±rala: ilk eleman en eski (ilk oluÅŸturulan)
                        candidates.sort(key=lambda p: os.path.getmtime(p))
                        timestamp_dir = candidates[0]
                        print(f"ğŸ•’ Ä°lk timestamp kuralÄ±: mevcutlardan EN ESKÄ°SÄ° kullanÄ±lacak â†’ {os.path.basename(timestamp_dir)}")
            except Exception:
                pass
            # 3) HiÃ§biri yoksa yeni timestamp oluÅŸtur
            if not timestamp_dir:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                timestamp_dir = os.path.join(base_input, timestamp)
            checkpoints_dir = os.path.join(timestamp_dir, 'checkpoints')
            models_dir = os.path.join(timestamp_dir, 'models')
            logs_dir = os.path.join(timestamp_dir, 'logs')
            configs_dir = os.path.join(timestamp_dir, 'configs')

            try:
                created_any = False
                for d in [checkpoints_dir, models_dir, logs_dir, configs_dir]:
                    if not os.path.isdir(d):
                        os.makedirs(d, exist_ok=True)
                        created_any = True
                action = "kullanÄ±lÄ±yor" if not created_any else "hazÄ±rlandÄ±"
                print(f"âœ… Drive timestamp {action}: {timestamp_dir}")
                print(f"ğŸ—‚ï¸  KayÄ±t hedefi (checkpoints): {checkpoints_dir}")
                # EÄŸitim opsiyonlarÄ±nda doÄŸrudan 'checkpoints' klasÃ¶rÃ¼nÃ¼ hedefle
                drive_save_path = checkpoints_dir
                # Etiket modu 2 ise: sÄ±nÄ±f ID listesini configs/ altÄ±na yaz (varsa)
                try:
                    if (dataset_config.get('type') == 'hierarchical_multi' and
                        (dataset_config.get('setup') or {}).get('label_mode') == 'preserve_subclasses'):
                        _write_class_ids_json(configs_dir)
                except Exception:
                    pass
            except Exception as e:
                print(f"âŒ Drive klasÃ¶rleri oluÅŸturulamadÄ±: {e}")
                drive_save_path = None
    
    # EÄŸitim parametreleri (gÃ¶rÃ¼ntÃ¼ boyutu -> batch -> epoch)
    # Model size selection (tek soru)
    print("\nModel boyutunu seÃ§in:")
    print("1) yolo11s.pt - KÃ¼Ã§Ã¼k (en hÄ±zlÄ±, dÃ¼ÅŸÃ¼k doÄŸruluk)")
    print("2) yolo11m.pt - Orta (dengeli)")
    print("3) yolo11l.pt - BÃ¼yÃ¼k (yÃ¼ksek doÄŸruluk, yavaÅŸ) [HiyerarÅŸik iÃ§in Ã¶nerilen]")
    print("4) yolo11x.pt - Ã‡ok BÃ¼yÃ¼k (en yÃ¼ksek doÄŸruluk, en yavaÅŸ)")

    while True:
        model_choice = input("\nModel seÃ§in [1-4] (varsayÄ±lan: 3): ") or "3"
        
        model_options = {
            "1": "yolo11s.pt",
            "2": "yolo11m.pt",
            "3": "yolo11l.pt",
            "4": "yolo11x.pt"
        }
        
        if model_choice in model_options:
            model = model_options[model_choice]
            
            # Check if model exists locally/Drive
            if is_colab():
                model_dir = get_smartfarm_models_dir() or os.path.join("/content/colab_learn", "yolo11_models")
            else:
                model_dir = "yolo11_models"
            model_path = os.path.join(model_dir, model)
            
            if not os.path.exists(model_path):
                print(f"\nâš ï¸  Model {model} yerel olarak bulunamadÄ±.")
                download_now = input("Åimdi indir? (e/h, varsayÄ±lan: e): ").lower() or "e"
                
                if download_now.startswith("e"):
                    os.makedirs(model_dir, exist_ok=True)
                    download_specific_model_type("detection", model[6], model_dir)
                else:
                    print(f"â„¹ï¸  Model eÄŸitim sÄ±rasÄ±nda otomatik olarak indirilecek.")
            break
        print("âŒ LÃ¼tfen 1-4 arasÄ± seÃ§in.")
    
    # Batch size ve image size varsayÄ±lanlarÄ± (Colab iÃ§in optimize)
    # Ã–neri: batch_size=16, img_size=512 (RAM ve hÄ±z dengesi)
    default_batch = 16
    default_img_size = 512

    # Ã–nce gÃ¶rÃ¼ntÃ¼ boyutu
    while True:
        try:
            img_size = int(input(f"\nGÃ¶rÃ¼ntÃ¼ boyutu (varsayÄ±lan: {default_img_size}, 32'nin katÄ± olmalÄ± â€¢ Colab iÃ§in 512 Ã¶nerilir): ") or str(default_img_size))
            if img_size > 0 and img_size % 32 == 0:
                break
            print("âŒ LÃ¼tfen 32'nin katÄ± olan pozitif bir sayÄ± girin.")
        except ValueError:
            print("âŒ LÃ¼tfen geÃ§erli bir sayÄ± girin.")

    # Sonra batch boyutu
    while True:
        try:
            batch_size = int(input(f"\nBatch boyutu (varsayÄ±lan: {default_batch}, dÃ¼ÅŸÃ¼k RAM iÃ§in kÃ¼Ã§Ã¼k): ") or str(default_batch))
            if batch_size > 0:
                break
            print("âŒ LÃ¼tfen pozitif bir sayÄ± girin.")
        except ValueError:
            print("âŒ LÃ¼tfen geÃ§erli bir sayÄ± girin.")

    # En son epoch
    while True:
        try:
            default_epochs = 1000
            epochs = int(input(f"\nEpoch sayÄ±sÄ± [100-2000 Ã¶nerilen] (varsayÄ±lan: {default_epochs}): ") or str(default_epochs))
            if epochs > 0:
                break
            print("âŒ LÃ¼tfen pozitif bir sayÄ± girin.")
        except ValueError:
            print("âŒ LÃ¼tfen geÃ§erli bir sayÄ± girin.")

    # Speed mode (optimize epoch time)
    speed_mode_input = (input("\nHÄ±z modu (cache=ram, workers=8, plots=False) aÃ§Ä±lsÄ±n mÄ±? (e/h, varsayÄ±lan: e): ") or "e").lower()
    speed_mode = speed_mode_input.startswith('e')
    
    # Hyperparameter file
    use_hyp = input("\nHiperparametre dosyasÄ± kullan (hyp.yaml)? (e/h, varsayÄ±lan: e): ").lower() or "e"
    use_hyp = use_hyp.startswith("e")
    
    # Device detection
    device = check_gpu()
    
    # Build options dictionary
    options = {
        'dataset_config': dataset_config,
        'epochs': epochs,
        'model': model,
        'batch': batch_size,
        'imgsz': img_size,
        'device': device,
        'workers': None,
        'data': dataset_config['data_yaml'],
        'project': 'runs/train',
        'name': 'exp',
        'pretrained': True,
        'optimizer': 'auto',
        'verbose': True,
        'exist_ok': True,
        'use_hyp': use_hyp,
        'category': category,
        'drive_save_path': drive_save_path,
        'speed_mode': speed_mode
    }
    # --- Veri indirme baÅŸlamadan Ã–NCE: Otomatik checkpoint aramasÄ± ve kullanÄ±cÄ±ya bilgi vererek seÃ§im alma ---
    try:
        if is_colab() and _DRIVE_AVAILABLE:
            dm = DriveManager()
            if dm.authenticate():
                # KonfigÃ¼rasyon varsa yÃ¼kle, yoksa yine de arama yap (projeyi bilmeden de base dizinde arÄ±yor)
                try:
                    if hasattr(dm, 'load_drive_config'):
                        dm.load_drive_config()
                except Exception:
                    pass
                print("\nğŸ” Drive'da mevcut checkpoint aranÄ±yor (en yeni timestamp'tan geriye doÄŸru)...")
                ckpt_path, ckpt_name = dm.find_latest_checkpoint()
                if ckpt_path:
                    print(f"âœ… Bulundu: {ckpt_name}\nğŸ“„ Yol: {ckpt_path}")
                    ask = (input("KaldÄ±ÄŸÄ± yerden devam edilsin mi? (e/h, varsayÄ±lan: e): ") or "e").lower()
                    if ask.startswith('e'):
                        options['resume'] = True
                        options['checkpoint_path'] = ckpt_path
                        print("ğŸ”„ EÄŸitim, veri indirme adÄ±mÄ± atlanarak checkpoint'ten devam edecek.")
                    else:
                        print("â„¹ï¸ Resume iptal edildi. Yeni eÄŸitim kurulumu ile devam edilecek.")
                else:
                    print("â„¹ï¸ Drive'da kullanÄ±labilir checkpoint bulunamadÄ±. Yeni eÄŸitim kurulumu ile devam edilecek.")
            else:
                print("âš ï¸ Drive mount/kimlik doÄŸrulama baÅŸarÄ±sÄ±z. Resume aramasÄ± yapÄ±lamadÄ±.")
        else:
            if not is_colab():
                print("â„¹ï¸ Colab ortamÄ± deÄŸil. Drive tabanlÄ± otomatik resume aramasÄ± atlandÄ±.")
            elif not _DRIVE_AVAILABLE:
                print("âš ï¸ drive_manager iÃ§e aktarÄ±lamadÄ±. Drive tabanlÄ± otomatik resume aramasÄ± yapÄ±lamadÄ±.")
    except Exception as pre_resume_err:
        print(f"âš ï¸ Otomatik resume kontrolÃ¼ sÄ±rasÄ±nda hata: {pre_resume_err}")

    
    # Display selected parameters
    print("\n===== SeÃ§ilen EÄŸitim Parametreleri =====")
    print(f"Veri seti tipi: {dataset_config['type']}")
    if dataset_config['type'] == 'hierarchical_multi':
        setup = dataset_config['setup']
        print(f"Veri seti grubu: {setup['selected_group']}")
        print(f"SÄ±nÄ±f baÅŸÄ±na hedef Ã¶rnek: {setup['target_count']:,}")
        print(f"Ã‡Ä±ktÄ± dizini: {setup['output_dir']}")
    
    print(f"Model: {model}")
    print(f"Epoch: {epochs}")
    print(f"Batch boyutu: {batch_size}")
    print(f"GÃ¶rÃ¼ntÃ¼ boyutu: {img_size}")
    print(f"Cihaz: {device}")
    print(f"DataLoader workers: {options['workers']} (hafÄ±za iÃ§in dÃ¼ÅŸÃ¼k)")
    print(f"Dataset cache varsayÄ±lanÄ±: {'ram' if speed_mode else 'disk'}")
    print(f"cuDNN benchmark: Enabled (training.py iÃ§inde)")
    print(f"HÄ±z modu: {'AÃ§Ä±k' if speed_mode else 'KapalÄ±'}")
    print(f"Kategori: {category}")
    if dataset_config['type'] == 'hierarchical_multi':
        pct = dataset_config['setup'].get('per_class_targets')
        if pct:
            print("SÄ±nÄ±f bazlÄ± hedefler: (Ã¶zet)")
            shown = 0
            for k, v in pct.items():
                print(f"  â€¢ {k}: {v}")
                shown += 1
                if shown >= 10:
                    print("  â€¢ ... (daha fazla sÄ±nÄ±f var)")
                    break
    # Kaydetme aralÄ±ÄŸÄ± sorusu training.py iÃ§inde (menÃ¼lÃ¼) yÃ¶netiliyor
    
    if drive_save_path:
        print(f"Drive kaydetme yolu: {drive_save_path}")
        # Kaydedilecek dosyalarÄ± net belirt (checkpoints altÄ±nda)
        print(f"Kaydedilecek dosyalar:")
        print(f"  â€¢ best.pt  â†’ {os.path.join(drive_save_path, 'best.pt')}")
        print(f"  â€¢ last.pt  â†’ {os.path.join(drive_save_path, 'last.pt')}")
    
    confirm = (input("\nBu parametrelerle devam et? (e/h, varsayÄ±lan: e): ") or "e").lower()
    if confirm != 'e' and confirm != 'evet' and confirm != 'yes':
        print("âŒ Kurulum iptal edildi.")
        return None
    
    return options

def get_dataset_split_config(api_key):
    """API key varsa train/test/val deÄŸerlerini al"""
    if not api_key:
        return None
    
    print("\nğŸ“Š Dataset BÃ¶lÃ¼mleme AyarlarÄ±")
    print("=" * 40)
    print("API key mevcut - dataset bÃ¶lÃ¼mleme ayarlarÄ±nÄ± yapÄ±landÄ±rabilirsiniz")
    
    use_custom_split = input("\nÃ–zel train/test/val oranÄ± kullanmak istiyor musunuz? (e/h, varsayÄ±lan: h): ").lower() or "h"
    
    if not use_custom_split.startswith('e'):
        print("âœ… VarsayÄ±lan bÃ¶lÃ¼mleme kullanÄ±lacak")
        return None
    
    print("\nğŸ“‹ BÃ¶lÃ¼mleme OranÄ± GiriÅŸi:")
    print("Not: Toplam 100 olmalÄ± (train + test + val = 100)")
    
    while True:
        try:
            train_pct = int(input("Train oranÄ± (varsayÄ±lan: 70): ") or "70")
            test_pct = int(input("Test oranÄ± (varsayÄ±lan: 20): ") or "20")
            val_pct = int(input("Validation oranÄ± (varsayÄ±lan: 10): ") or "10")
            
            total = train_pct + test_pct + val_pct
            if total != 100:
                print(f"âŒ Toplam {total}%. LÃ¼tfen toplamÄ± 100 yapacak ÅŸekilde girin.")
                continue
            
            if train_pct < 50:
                print("âš ï¸ Train oranÄ± %50'den az. Devam etmek istiyor musunuz? (e/h): ", end="")
                if not input().lower().startswith('e'):
                    continue
            
            split_config = {
                'train': train_pct,
                'test': test_pct, 
                'val': val_pct
            }
            
            print(f"\nâœ… BÃ¶lÃ¼mleme ayarlarÄ±: Train %{train_pct}, Test %{test_pct}, Val %{val_pct}")
            return split_config
            
        except ValueError:
            print("âŒ LÃ¼tfen geÃ§erli sayÄ±lar girin")
            continue

def handle_roboflow_api_management(url):
    """Roboflow API key yÃ¶netimini handle et"""
    print("\nğŸ”‘ Roboflow API YÃ¶netimi")
    print("=" * 40)
    
    # Mevcut API key kontrol et
    existing_key = get_api_key_from_config()
    if existing_key:
        print(f"âœ… Mevcut API key bulundu: {existing_key[:10]}...")
        use_existing = input("Mevcut API key'i kullanmak istiyor musunuz? (e/h, varsayÄ±lan: e): ").lower() or "e"
        if use_existing.startswith('e'):
            # API key varsa split config al
            split_config = get_dataset_split_config(existing_key)
            return {'api_key': existing_key, 'split_config': split_config}
    
    print("\nğŸ“‹ SeÃ§enekler:")
    print("1) API Key gir (train/test/val ayarlarÄ± ile)")
    print("2) API Key olmadan devam et (public dataset)")
    
    while True:
        choice = input("\nSeÃ§enek [1-2] (varsayÄ±lan: 2): ").strip() or "2"
        
        if choice == "2":
            print("âœ… API key olmadan devam ediliyor (public dataset olarak)")
            return {'api_key': None, 'split_config': None}
        
        elif choice == "1":
            print("\nğŸ“‹ API Key alma adÄ±mlarÄ±:")
            print("1. https://roboflow.com adresine gidin")
            print("2. HesabÄ±nÄ±za giriÅŸ yapÄ±n")
            print("3. Settings > API sayfasÄ±na gidin")
            print("4. Private API Key'inizi kopyalayÄ±n")
            
            api_key = input("\nğŸ”‘ API Key'inizi girin (boÅŸ bÄ±rakabilirsiniz): ").strip()
            
            if api_key:
                # API key'i kaydet
                result = handle_roboflow_action('1', api_key=api_key)
                if result:
                    print("âœ… API key baÅŸarÄ±yla kaydedildi!")
                    # Split config al
                    split_config = get_dataset_split_config(api_key)
                    return {'api_key': api_key, 'split_config': split_config}
                else:
                    print("âŒ API key kaydedilemedi, boÅŸ olarak devam ediliyor")
                    return {'api_key': None, 'split_config': None}
            else:
                print("âœ… API key boÅŸ bÄ±rakÄ±ldÄ±, public dataset olarak devam ediliyor")
                return {'api_key': None, 'split_config': None}
        
        else:
            print("âŒ GeÃ§ersiz seÃ§enek")
            continue

def main():
    """Main function - Hierarchical Multi-Dataset Training Framework"""
    # Language selection at startup
    select_language()
    
    # --- Dil seÃ§iminden hemen sonra: Drive timestamp oturumunu sabitle (global) ---
    try:
        # Sadece Colab'de anlamlÄ±; fakat kod gÃ¼venle Ã§alÄ±ÅŸÄ±r
        from drive_manager import activate_drive_integration
        import yaml as _yaml
        # drive kÃ¶kÃ¼nÃ¼ config'ten oku
        drive_folder = "SmartFarm/colab_learn/yolo11_models"
        try:
            _cfg_path = 'config_datasets.yaml'
            if os.path.exists(_cfg_path):
                with open(_cfg_path, 'r', encoding='utf-8') as _cf:
                    _cfg = _yaml.safe_load(_cf) or {}
                _gs = _cfg.get('global_settings', {}) if isinstance(_cfg, dict) else {}
                _p = _gs.get('drive_folder_path')
                if isinstance(_p, str) and _p.strip():
                    drive_folder = _p.strip()
        except Exception:
            pass
        dm = activate_drive_integration(folder_path=drive_folder, project_name="yolo11_models")
        if dm and getattr(dm, 'project_folder', None):
            # Env deÄŸiÅŸkenine yaz ki tÃ¼m sÃ¼reÃ§ler aynÄ± timestamp'i kullansÄ±n
            os.environ['SMARTFARM_DRIVE_TS'] = dm.project_folder
            print(f"ğŸŒ Global Drive session: {dm.project_folder}")
    except Exception as _sess_e:
        print(f"âš ï¸ Drive session sabitleme atlandÄ±: {_sess_e}")
    
    # Drive baÄŸlantÄ± kontrolÃ¼ (dil seÃ§iminden sonra)
    try:
        from drive_manager import debug_colab_environment, manual_drive_mount
        
        # Colab ortamÄ±nda Drive kontrolÃ¼
        is_colab = debug_colab_environment()
        if is_colab:
            print(f"\n{get_text('drive_check_title', default='ğŸ” Google Drive BaÄŸlantÄ± KontrolÃ¼')}")
            print("="*50)
            
            # Drive mount durumu kontrol et
            import os
            if not os.path.exists('/content/drive/MyDrive'):
                print(f"{get_text('drive_not_mounted', default='âŒ Google Drive mount edilmemiÅŸ!')}")
                
                mount_choice = input(f"{get_text('mount_drive_question', default='Drive\'Ä± ÅŸimdi mount etmek ister misiniz? (e/h, varsayÄ±lan: e)')} ").lower() or "e"
                
                if mount_choice.startswith('e'):
                    if manual_drive_mount():
                        print(f"{get_text('drive_mount_success', default='âœ… Drive baÅŸarÄ±yla mount edildi!')}")
                    else:
                        print(f"{get_text('drive_mount_failed', default='âŒ Drive mount baÅŸarÄ±sÄ±z. EÄŸitim yerel kaydetme ile devam edecek.')}")
                else:
                    print(f"{get_text('drive_skip_info', default='â„¹ï¸ Drive mount atlandÄ±. EÄŸitim yerel kaydetme ile yapÄ±lacak.')}")
            else:
                print(f"{get_text('drive_already_mounted', default='âœ… Google Drive zaten mount edilmiÅŸ!')}")
                
    except ImportError:
        pass  # Drive manager mevcut deÄŸilse sessizce devam et
    except Exception as e:
        print(f"âš ï¸ Drive kontrol hatasÄ±: {e}")
    
    print("\n" + "="*70)
    print(get_text('main_title'))
    print(get_text('main_subtitle'))
    print("="*70)
    
    print(f"\n{get_text('main_menu')}")
    print(get_text('option_download'))
    print(get_text('option_training'))
    print(get_text('option_test'))
    print(get_text('option_exit'))
    
    choice = input(f"\n{get_text('select_option')}") or "1"
    
    if choice == "1":
        download_models_menu()
        if get_text('language_choice').startswith('e'):
            train_now = input("\nEÄŸitim kurulumuna geÃ§? (e/h, varsayÄ±lan: e): ").lower() or "e"
            if not train_now.startswith("e"):
                return
        else:
            train_now = input("\nProceed to training setup? (y/n, default: y): ").lower() or "y"
            if not train_now.startswith("y"):
                return
        choice = "2"  # Continue to training
        
    if choice == "2":
        in_colab = is_colab
        
        # (Opsiyonel) Gerekli paketleri yÃ¼kleme
        # Not: Paket kurulumlarÄ±nÄ± genellikle colab_setup.py Ã¼zerinden yÃ¶netmeniz Ã¶nerilir.
        do_install = (input("\nGerekli paketleri ÅŸimdi yÃ¼klemek ister misiniz? (e/h, varsayÄ±lan: h): ") or "h").lower()
        if do_install.startswith("e"):
            print("\nğŸ“¦ Gerekli paketler yÃ¼kleniyor...")
            install_required_packages()
        else:
            print("\nâ­ï¸ Paket yÃ¼kleme atlandÄ±. (colab_setup.py ile kurulumu yapabilirsiniz)")
        
        # Interactive setup - this will handle checkpoint checking
        options = interactive_training_setup()
        if options is None:
            return
        
        # EÄŸitim parametrelerini merkezi olarak normalize et
        options = prepare_training_options(options)
        
        # Check if we're resuming from a checkpoint
        if options.get('resume'):
            print("\n" + "="*50)
            print(f"ğŸ”„ EÄŸitime devam ediliyor: {options['checkpoint_path']}")
            print("="*50)

            # Resume'da veri YAML doÄŸrulamasÄ±: yoksa Drive'daki checkpoint klasÃ¶rÃ¼nden kullan
            try:
                yaml_path = options.get('data', 'merged_dataset.yaml')
                if not os.path.isabs(yaml_path):
                    local_yaml = os.path.join(os.getcwd(), yaml_path)
                else:
                    local_yaml = yaml_path

                if not os.path.exists(local_yaml):
                    ckpt_dir = os.path.dirname(options['checkpoint_path'])
                    drive_yaml = os.path.join(ckpt_dir, os.path.basename(yaml_path))
                    if os.path.exists(drive_yaml):
                        options['data'] = drive_yaml
                        print(f"â„¹ï¸ Yerelde '{yaml_path}' bulunamadÄ±. Drive'dan kullanÄ±lacak: {drive_yaml}")
                    else:
                        print(f"â— Gerekli data YAML bulunamadÄ±: '{yaml_path}'.")
                        print("   - Yerelde yok.")
                        print(f"   - Drive klasÃ¶rÃ¼nde de yok: {drive_yaml}")
                        # KullanÄ±cÄ±ya hÄ±zlÄ± Ã§Ã¶zÃ¼m: dataset iÅŸlemi Ã§alÄ±ÅŸtÄ±rÄ±lsÄ±n mÄ±?
                        do_process = (input("YAML'Ä± Ã¼retmek iÃ§in veri iÅŸleme adÄ±mÄ±nÄ± Ã§alÄ±ÅŸtÄ±ralÄ±m mÄ±? (e/h, varsayÄ±lan: e): ") or "e").lower()
                        if do_process.startswith('e'):
                            dc = options['dataset_config']
                            if dc['type'] == 'hierarchical_multi':
                                if not process_hierarchical_datasets(dc['setup']):
                                    print('âŒ Veri seti iÅŸleme baÅŸarÄ±sÄ±z. Ã‡Ä±kÄ±lÄ±yor...')
                                    return
                                # BaÅŸarÄ±lÄ±ysa yeniden yerel YAML'Ä± kullan
                                if os.path.exists(local_yaml):
                                    options['data'] = local_yaml
                                    print(f"âœ… YAML Ã¼retildi ve kullanÄ±lacak: {local_yaml}")
                            else:
                                print("âš ï¸ Bu modda otomatik YAML Ã¼retimi desteklenmiyor. LÃ¼tfen 'dataset.yaml' yolunu doÄŸru girin.")
                        else:
                            print("âŒ YAML olmadan eÄŸitime devam edilemez. Ã‡Ä±kÄ±lÄ±yor...")
                            return
            except Exception as yaml_check_err:
                print(f"âš ï¸ Resume Ã¶ncesi YAML kontrolÃ¼nde hata: {yaml_check_err}")

            # Skip dataset processing when resuming (YAML doÄŸrulamasÄ± yapÄ±ldÄ±)
            results = train_model(options, hyp=None, epochs=options['epochs'])
        else:
            # Process dataset(s) for new training
            dataset_config = options['dataset_config']
            
            if dataset_config['type'] == 'single':
                # Single dataset processing (legacy) - API key ve split config ile
                from dataset_utils import download_dataset
                
                api_key = dataset_config.get('api_key')
                split_config = dataset_config.get('split_config')
                
                if not download_dataset(dataset_config['url'], api_key=api_key, split_config=split_config):
                    print('âŒ Veri seti indirme baÅŸarÄ±sÄ±z. Ã‡Ä±kÄ±lÄ±yor...')
                    return
                    
            elif dataset_config['type'] == 'hierarchical_multi':
                # Hierarchical multi-dataset processing
                if not process_hierarchical_datasets(dataset_config['setup']):
                    print('âŒ HiyerarÅŸik veri seti iÅŸleme baÅŸarÄ±sÄ±z. Ã‡Ä±kÄ±lÄ±yor...')
                    return
                # SeÃ§enek 2 ise eÄŸitimde config/merged_class_dataset.yaml kullanÄ±lmalÄ±
                try:
                    lm = (dataset_config.get('setup') or {}).get('label_mode')
                    if lm == 'preserve_subclasses':
                        options['data'] = os.path.join('config', 'merged_class_dataset.yaml')
                        print(f"â„¹ï¸ EÄŸitim YAML: {options['data']}")
                except Exception:
                    pass
            
            # Show memory status before training
            show_memory_usage("EÄŸitim Ã–ncesi")
            
            # Create hyperparameter file for new training
            from hyperparameters import create_hyperparameters_file, load_hyperparameters
            hyp_path = create_hyperparameters_file()
            hyperparameters = load_hyperparameters(hyp_path)
            
            # Start new training
            print(f"\nğŸš€ Yeni model eÄŸitimi baÅŸlatÄ±lÄ±yor...")
            # Normalize edilmiÅŸ options zaten mevcut; train_model'e aktar
            results = train_model(options, hyp=hyperparameters, 
                               epochs=options['epochs'], 
                               drive_save_interval=options.get('save_interval', 10))
        
        if results:
            print('âœ… EÄŸitim baÅŸarÄ±yla tamamlandÄ±!')
            print(f'ğŸ“Š SonuÃ§lar: {results}')
            
            # Initialize hierarchical detection if available
            if HIERARCHICAL_DETECTION_AVAILABLE:
                print(f"\nğŸ¯ HiyerarÅŸik tespit sistemi baÅŸlatÄ±lÄ±yor...")
                try:
                    visualizer = HierarchicalDetectionVisualizer()
                    print(f"âœ… HiyerarÅŸik tespit sistemi hazÄ±r!")
                    print(f"ğŸ·ï¸  Tespit formatÄ±: 'ZARARLI: KÄ±rmÄ±zÄ± Ã–rÃ¼mcek (0.85)'")
                except Exception as e:
                    print(f"âš ï¸  HiyerarÅŸik tespit baÅŸlatÄ±lamadÄ±: {e}")
            
            # Save to Google Drive (otomatik kaydetme - tekrar soru sorma)
            if in_colab and options.get('drive_save_path'):
                drive_path = options['drive_save_path']
                print(f"\nğŸ’¾ Modeller Google Drive'a kaydediliyor...")
                print(f"ğŸ“ Hedef klasÃ¶r: {drive_path}")
                if save_models_to_drive(drive_path):
                    print(f"âœ… Modeller baÅŸarÄ±yla kaydedildi: {drive_path}")
                    print(f"ğŸ“‚ Kaydedilen dosyalar ÅŸu konumda: {drive_path}")
                else:
                    print("âŒ Modeller Google Drive'a kaydedilemedi.")
        else:
            print('âŒ EÄŸitim baÅŸarÄ±sÄ±z veya kesildi.')
            
            # Save partial results if available (otomatik kaydetme)
            if in_colab and options.get('drive_save_path'):
                drive_path = options['drive_save_path']
                print(f"\nğŸ’¾ KÄ±smi sonuÃ§lar Google Drive'a kaydediliyor...")
                print(f"ğŸ“ Hedef klasÃ¶r: {drive_path}")
                if save_models_to_drive(drive_path):
                    print(f"âœ… KÄ±smi sonuÃ§lar kaydedildi: {drive_path}")
                    print(f"ğŸ“‚ Kaydedilen dosyalar ÅŸu konumda: {drive_path}")
                else:
                    print("âŒ KÄ±smi sonuÃ§lar kaydedilemedi.")
        
        # Clean memory
        show_memory_usage("EÄŸitim SonrasÄ±")
        clean_memory()
    
    elif choice == "3":
        # Test hierarchical detection
        if not HIERARCHICAL_DETECTION_AVAILABLE:
            print("âŒ HiyerarÅŸik tespit araÃ§larÄ± mevcut deÄŸil.")
            return
        
        model_path = input("EÄŸitilmiÅŸ model yolunu girin (varsayÄ±lan: runs/train/exp/weights/best.pt): ").strip() or "runs/train/exp/weights/best.pt"
        if not model_path or not os.path.exists(model_path):
            print("âŒ Model dosyasÄ± bulunamadÄ±.")
            return
        
        test_image = input("Test gÃ¶rÃ¼ntÃ¼sÃ¼ yolunu girin (varsayÄ±lan: test.jpg): ").strip() or "test.jpg"
        if not test_image or not os.path.exists(test_image):
            print("âŒ Test gÃ¶rÃ¼ntÃ¼sÃ¼ bulunamadÄ±.")
            return
        
        try:
            from ultralytics import YOLO
            import cv2
            
            # Load model and visualizer
            model = YOLO(model_path)
            visualizer = HierarchicalDetectionVisualizer()
            
            # Run detection
            print(f"ğŸ” HiyerarÅŸik tespit Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
            image = cv2.imread(test_image)
            results = model(image)
            
            # Apply hierarchical visualization
            annotated_image = visualizer.process_yolo_results(image, results[0])
            
            # Save result
            output_path = "hierarchical_detection_result.jpg"
            cv2.imwrite(output_path, annotated_image)
            
            # Generate report
            detections = visualizer.get_detection_summary(results[0])
            report = visualizer.create_detection_report(detections)
            
            print(f"\n{report}")
            print(f"âœ… AÃ§Ä±klamalÄ± gÃ¶rÃ¼ntÃ¼ kaydedildi: {output_path}")
            
        except Exception as e:
            print(f"âŒ HiyerarÅŸik tespit testi sÄ±rasÄ±nda hata: {e}")
    
    elif choice == "4":
        print("ğŸ‘‹ Ã‡Ä±kÄ±lÄ±yor...")
    
    else:
        print("âŒ GeÃ§ersiz seÃ§enek. Ã‡Ä±kÄ±lÄ±yor...")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸  KullanÄ±cÄ± tarafÄ±ndan kesildi. Ã‡Ä±kÄ±lÄ±yor...")
    except Exception as e:
        print(f"\nâŒ Bir hata oluÅŸtu: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("\nâœ… Ä°ÅŸlem tamamlandÄ±.")
