#!/usr/bin/env python3
# main_multi_dataset.py - Hiyerar≈üik √ßoklu dataset y√∂netimi ve eƒüitim

import os
import sys
import yaml
from pathlib import Path
import subprocess

# Roboflow API y√∂netimi i√ßin import
try:
    from roboflow_api_helper import get_roboflow_menu_choice, handle_roboflow_action, get_api_key_from_config
except ImportError:
    print("‚ö†Ô∏è roboflow_api_helper.py bulunamadƒ±")
    def get_roboflow_menu_choice(): return {}
    def handle_roboflow_action(choice, **kwargs): return False
    def get_api_key_from_config(): return None
import shutil
from datetime import datetime
import json
import re

# Import framework components
from setup_utils import check_gpu, install_required_packages
from hyperparameters import create_hyperparameters_file, load_hyperparameters
from memory_utils import show_memory_usage, clean_memory
from training import train_model, save_to_drive
from training_optimizer import prepare_training_options
try:
    from drive_manager import DriveManager
    _DRIVE_AVAILABLE = True
except Exception:
    _DRIVE_AVAILABLE = False
from model_downloader import download_yolo11_models, download_specific_model_type
from language_manager import get_text, select_language

# Import updated multi-dataset manager
try:
    from multi_dataset_manager import YAMLBasedMultiDatasetManager
except ImportError:
    print("‚ö†Ô∏è  YAMLBasedMultiDatasetManager not found, trying legacy import...")
    from multi_dataset_manager import MultiDatasetManager as YAMLBasedMultiDatasetManager

# Import hierarchical detection utils
try:
    from hierarchical_detection_utils import HierarchicalDetectionVisualizer
    HIERARCHICAL_DETECTION_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  HierarchicalDetectionVisualizer not available")
    HIERARCHICAL_DETECTION_AVAILABLE = False

# Import augmentation systems
try:
    from augmentation import TomatoDiseaseAugmentation
    from augmentation import TomatoPestAugmentation
    AUGMENTATION_SYSTEMS_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  Augmentation systems not available")
    AUGMENTATION_SYSTEMS_AVAILABLE = False

# Ek: Hedefe-tamamlama i√ßin temel augmentation pipeline (opsiyonel)
try:
    from augmentation_utils import YOLOAugmentationPipeline
    _AUG_PIPE_AVAILABLE = True
except Exception:
    _AUG_PIPE_AVAILABLE = False

# Global timestamp pattern for consistent naming
TIMESTAMP_PATTERN = re.compile(r'^\d{8}_\d{6}$')  # YYYYMMDD_HHMMSS format

# Global timestamp variable
_GLOBAL_TIMESTAMP = None

def get_global_timestamp():
    """Get or create global timestamp for consistent naming across the project"""
    global _GLOBAL_TIMESTAMP
    if _GLOBAL_TIMESTAMP is None:
        _GLOBAL_TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")
    return _GLOBAL_TIMESTAMP

def save_user_preferences_config(options, dataset_config=None, augmentation_settings=None):
    """Kullanƒ±cƒ± tercihlerini ve ayarlarƒ±nƒ± configs klas√∂r√ºnde kaydet"""
    try:
        # Global timestamp ile config klas√∂r√º olu≈ütur
        global_ts = get_global_timestamp()
        configs_dir = os.path.join('configs', global_ts)
        os.makedirs(configs_dir, exist_ok=True)
        
        # Ana konfig√ºrasyon dosyasƒ±
        config_data = {
            'session_info': {
                'timestamp': global_ts,
                'created_at': datetime.now().isoformat(),
                'environment': 'colab' if is_colab() else 'local',
                'language': get_text('language_choice', default='tr')
            },
            'training_settings': {
                'model': options.get('model'),
                'epochs': options.get('epochs'),
                'batch_size': options.get('batch'),
                'image_size': options.get('imgsz'),
                'device': options.get('device'),
                'workers': options.get('workers'),
                'optimizer': options.get('optimizer'),
                'speed_mode': options.get('speed_mode'),
                'use_hyperparameters': options.get('use_hyp'),
                'category': options.get('category'),
                'project_path': options.get('project'),
                'experiment_name': options.get('name')
            },
            'dataset_configuration': {},
            'drive_settings': {
                'save_to_drive': bool(options.get('drive_save_path')),
                'drive_path': options.get('drive_save_path'),
                'save_interval': options.get('save_interval')
            },
            'augmentation_settings': augmentation_settings or {}
        }
        
        # Dataset konfig√ºrasyonu detaylarƒ±
        if dataset_config:
            config_data['dataset_configuration'] = {
                'type': dataset_config.get('type'),
                'data_yaml': dataset_config.get('data_yaml')
            }
            
            if dataset_config['type'] == 'hierarchical_multi':
                setup = dataset_config.get('setup', {})
                config_data['dataset_configuration'].update({
                    'selected_group': setup.get('selected_group'),
                    'target_count_per_class': setup.get('target_count'),
                    'per_class_targets': setup.get('per_class_targets'),
                    'output_directory': setup.get('output_dir'),
                    'label_mode': setup.get('label_mode'),
                    'recommendations': setup.get('recommendations')
                })
            elif dataset_config['type'] == 'single':
                config_data['dataset_configuration'].update({
                    'roboflow_url': dataset_config.get('url'),
                    'has_api_key': bool(dataset_config.get('api_key')),
                    'split_config': dataset_config.get('split_config')
                })
        
        # Ana config dosyasƒ±nƒ± kaydet
        main_config_path = os.path.join(configs_dir, 'training_session_config.json')
        with open(main_config_path, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Kullanƒ±cƒ± tercihleri kaydedildi: {main_config_path}")
        
        # √ñzet dosyasƒ± olu≈ütur (okunabilir format)
        summary_path = os.path.join(configs_dir, 'session_summary.txt')
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(f"SmartFarm Eƒüitim Oturumu √ñzeti\n")
            f.write(f"{'='*50}\n\n")
            f.write(f"Oturum Bilgileri:\n")
            f.write(f"  ‚Ä¢ Timestamp: {global_ts}\n")
            f.write(f"  ‚Ä¢ Olu≈üturulma: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"  ‚Ä¢ Ortam: {'Google Colab' if is_colab() else 'Yerel'}\n")
            f.write(f"  ‚Ä¢ Dil: {config_data['session_info']['language']}\n\n")
            
            f.write(f"Eƒüitim Ayarlarƒ±:\n")
            f.write(f"  ‚Ä¢ Model: {options.get('model')}\n")
            f.write(f"  ‚Ä¢ Epoch: {options.get('epochs')}\n")
            f.write(f"  ‚Ä¢ Batch Boyutu: {options.get('batch')}\n")
            f.write(f"  ‚Ä¢ G√∂r√ºnt√º Boyutu: {options.get('imgsz')}\n")
            f.write(f"  ‚Ä¢ Cihaz: {options.get('device')}\n")
            f.write(f"  ‚Ä¢ Hƒ±z Modu: {'A√ßƒ±k' if options.get('speed_mode') else 'Kapalƒ±'}\n")
            f.write(f"  ‚Ä¢ Kategori: {options.get('category')}\n\n")
            
            if dataset_config:
                f.write(f"Veri Seti Konfig√ºrasyonu:\n")
                f.write(f"  ‚Ä¢ Tip: {dataset_config.get('type')}\n")
                if dataset_config['type'] == 'hierarchical_multi':
                    setup = dataset_config.get('setup', {})
                    f.write(f"  ‚Ä¢ Se√ßilen Grup: {setup.get('selected_group')}\n")
                    f.write(f"  ‚Ä¢ Sƒ±nƒ±f Ba≈üƒ±na Hedef: {setup.get('target_count')}\n")
                    f.write(f"  ‚Ä¢ Etiket Modu: {setup.get('label_mode')}\n")
                f.write(f"\n")
            
            if options.get('drive_save_path'):
                f.write(f"Google Drive Ayarlarƒ±:\n")
                f.write(f"  ‚Ä¢ Kaydetme: A√ßƒ±k\n")
                f.write(f"  ‚Ä¢ Yol: {options.get('drive_save_path')}\n")
                f.write(f"  ‚Ä¢ Kaydetme Aralƒ±ƒüƒ±: {options.get('save_interval', 'Varsayƒ±lan')} epoch\n\n")
            
            if augmentation_settings:
                f.write(f"Augmentation Ayarlarƒ±:\n")
                for key, value in augmentation_settings.items():
                    f.write(f"  ‚Ä¢ {key}: {value}\n")
        
        print(f"üìÑ Oturum √∂zeti olu≈üturuldu: {summary_path}")
        
        return configs_dir
        
    except Exception as e:
        print(f"‚ö†Ô∏è Kullanƒ±cƒ± tercihleri kaydedilemedi: {e}")
        return None

def set_global_timestamp(timestamp):
    """Set global timestamp (used when user chooses existing timestamp)"""
    global _GLOBAL_TIMESTAMP
    _GLOBAL_TIMESTAMP = timestamp

def save_augmentation_config(configs_dir, augmentation_settings):
    """Augmentation ayarlarƒ±nƒ± ayrƒ± dosyada kaydet"""
    try:
        if not configs_dir or not augmentation_settings:
            return
        
        aug_config_path = os.path.join(configs_dir, 'augmentation_config.json')
        with open(aug_config_path, 'w', encoding='utf-8') as f:
            json.dump(augmentation_settings, f, indent=2, ensure_ascii=False)
        
        print(f"üé® Augmentation ayarlarƒ± kaydedildi: {aug_config_path}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Augmentation ayarlarƒ± kaydedilemedi: {e}")

def collect_augmentation_settings(dataset_config):
    """Kullanƒ±cƒ±nƒ±n augmentation tercihlerini topla"""
    augmentation_settings = {
        'enabled': False,
        'target_completion': False,
        'settings': {}
    }
    
    try:
        # Hierarchical multi-dataset i√ßin augmentation ayarlarƒ±
        if dataset_config and dataset_config.get('type') == 'hierarchical_multi':
            setup = dataset_config.get('setup', {})
            
            # Hedef tamamlama augmentation bilgileri
            if setup.get('target_count'):
                augmentation_settings.update({
                    'enabled': True,
                    'target_completion': True,
                    'settings': {
                        'target_count_per_class': setup.get('target_count'),
                        'per_class_targets': setup.get('per_class_targets'),
                        'copy_val_test': True,  # Varsayƒ±lan
                        'image_size': setup.get('settings', {}).get('default_image_size', 640)
                    }
                })
        
        # Environment'dan augmentation bilgilerini al
        env_aug_settings = os.environ.get('SMARTFARM_AUG_SETTINGS')
        if env_aug_settings:
            try:
                env_settings = json.loads(env_aug_settings)
                augmentation_settings['settings'].update(env_settings)
            except:
                pass
        
    except Exception as e:
        print(f"‚ö†Ô∏è Augmentation ayarlarƒ± toplanƒ±rken hata: {e}")
    
    return augmentation_settings

def find_existing_timestamps(base_dir):
    """Find existing timestamp directories in base directory"""
    if not os.path.exists(base_dir):
        return []
    
    timestamps = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path) and TIMESTAMP_PATTERN.match(item):
            timestamps.append({
                'name': item,
                'path': item_path,
                'mtime': os.path.getmtime(item_path)
            })
    
    # Sort by creation time (oldest first)
    timestamps.sort(key=lambda x: x['mtime'])
    return timestamps

def ask_user_for_timestamp_choice(language_choice='tr'):
    """Ask user whether to use existing timestamp or create new one"""
    # Check common directories for existing timestamps
    check_dirs = []
    
    # Add Google Drive paths if in Colab
    if is_colab():
        drive_paths = get_tarim_drive_paths()
        if drive_paths:
            check_dirs.extend([
                drive_paths['yolo11_models'],
                os.path.join(drive_paths['base'], 'yolo11_models')
            ])
        
        smartfarm_dir = get_smartfarm_models_dir()
        if smartfarm_dir:
            check_dirs.append(smartfarm_dir)
    
    # Add local directories
    check_dirs.extend([
        'yolo11_models',
        'runs/train',
        'datasets',
        'checkpoints'
    ])
    
    # Find all existing timestamps
    all_timestamps = []
    for check_dir in check_dirs:
        timestamps = find_existing_timestamps(check_dir)
        for ts in timestamps:
            ts['source_dir'] = check_dir
            all_timestamps.append(ts)
    
    # Remove duplicates (same timestamp name)
    unique_timestamps = {}
    for ts in all_timestamps:
        if ts['name'] not in unique_timestamps or ts['mtime'] < unique_timestamps[ts['name']]['mtime']:
            unique_timestamps[ts['name']] = ts
    
    timestamps_list = list(unique_timestamps.values())
    timestamps_list.sort(key=lambda x: x['mtime'])  # Oldest first
    
    if not timestamps_list:
        # No existing timestamps found
        new_timestamp = get_global_timestamp()
        if language_choice.startswith('tr'):
            print(f"üïí Yeni timestamp olu≈üturuldu: {new_timestamp}")
        else:
            print(f"üïí New timestamp created: {new_timestamp}")
        return new_timestamp
    
    # Show existing timestamps to user
    if language_choice.startswith('tr'):
        print(f"\nüïí Mevcut timestamp(ler) algƒ±landƒ±:")
        for i, ts in enumerate(timestamps_list, 1):
            print(f"  {i}. {ts['name']} (kaynak: {ts['source_dir']})")
        
        print(f"\nSe√ßenekler:")
        print(f"  e) Mevcut timestamp kullan (en eski: {timestamps_list[0]['name']})")
        print(f"  y) Yeni timestamp olu≈ütur")
        
        choice = input(f"\nTercihiniz (e/y, varsayƒ±lan: e): ").strip().lower() or 'e'
    else:
        print(f"\nüïí Existing timestamp(s) detected:")
        for i, ts in enumerate(timestamps_list, 1):
            print(f"  {i}. {ts['name']} (source: {ts['source_dir']})")
        
        print(f"\nOptions:")
        print(f"  e) Use existing timestamp (oldest: {timestamps_list[0]['name']})")
        print(f"  n) Create new timestamp")
        
        choice = input(f"\nYour choice (e/n, default: e): ").strip().lower() or 'e'
    
    if choice.startswith('e'):
        # Use oldest existing timestamp
        selected_timestamp = timestamps_list[0]['name']
        set_global_timestamp(selected_timestamp)
        if language_choice.startswith('tr'):
            print(f"‚úÖ Mevcut timestamp kullanƒ±lacak: {selected_timestamp}")
        else:
            print(f"‚úÖ Using existing timestamp: {selected_timestamp}")
        return selected_timestamp
    else:
        # Create new timestamp
        new_timestamp = get_global_timestamp()
        if language_choice.startswith('tr'):
            print(f"‚úÖ Yeni timestamp olu≈üturuldu: {new_timestamp}")
        else:
            print(f"‚úÖ New timestamp created: {new_timestamp}")
        return new_timestamp

# Check if running in Colab
def is_colab():
    """Check if running in Google Colab"""
    try:
        import google.colab
        print("‚úÖ Google Colab environment detected.")
        return True
    except:
        print("üíª Running in local environment.")
        return False

def get_tarim_drive_paths():
    """Google Drive'da 'Tarƒ±m' taban klas√∂r√ºn√º ve alt klas√∂rlerini bul/olu≈ütur.
    D√∂n√º≈ü: {
      'base': '/content/drive/MyDrive/Tarƒ±m',
      'colab_egitim': '/content/drive/MyDrive/Tarim/colab_egitim',
      'yolo11_models': '/content/drive/MyDrive/Tarim/colab_egitim/yolo11_models'
    }
    """
    if not is_colab():
        return None
    # Drive mount kontrol√º
    if not os.path.exists('/content/drive'):
        if not mount_google_drive():
            return None
    mydrive = "/content/drive/MyDrive"
    # T√ºrk√ße 'Tarƒ±m' varsa onu kullan, yoksa 'Tarim' ya da olu≈ütur
    tarim_candidates = [os.path.join(mydrive, 'Tarƒ±m'), os.path.join(mydrive, 'Tarim')]
    base = None
    for c in tarim_candidates:
        if os.path.exists(c):
            base = c
            break
    if base is None:
        # √ñnce T√ºrk√ße karakterli klas√∂r√º olu≈üturmayƒ± dene
        base = tarim_candidates[0]
        try:
            os.makedirs(base, exist_ok=True)
        except Exception:
            base = tarim_candidates[1]
            os.makedirs(base, exist_ok=True)
    colab_egitim = os.path.join(base, 'colab_egitim')
    yolo11_models = os.path.join(base, 'yolo11_models')
    os.makedirs(colab_egitim, exist_ok=True)
    os.makedirs(yolo11_models, exist_ok=True)
    return {'base': base, 'colab_egitim': colab_egitim, 'yolo11_models': yolo11_models}

def get_smartfarm_models_dir():
    """Colab i√ßin model klas√∂r√ºn√º '/content/drive/MyDrive/SmartFarm/colab_learn/yolo11_models' olarak d√∂nd√ºr√ºr ve yoksa olu≈üturur."""
    if not is_colab():
        return None
    if not os.path.exists('/content/drive'):
        if not mount_google_drive():
            return None
    path = "/content/drive/MyDrive/SmartFarm/colab_learn/yolo11_models"
    os.makedirs(path, exist_ok=True)
    return path

def mount_google_drive():
    """Mount Google Drive"""
    try:
        from google.colab import drive
        drive.mount('/content/drive')
        print("‚úÖ Google Drive mounted successfully.")
        return True
    except Exception as e:
        print(f"‚ùå Error mounting Google Drive: {e}")
        return False

def save_models_to_drive(drive_folder_path, best_file=True, last_file=True):
    """Save best and last model files to Google Drive"""
    if not is_colab():
        print("‚ÑπÔ∏è  Bu fonksiyon sadece Google Colab'da √ßalƒ±≈üƒ±r.")
        return False
    
    # Check if Google Drive is mounted
    if not os.path.exists('/content/drive'):
        if not mount_google_drive():
            return False
    
    # Find the most recent training directory
    runs_dir = "runs/train"
    if not os.path.exists(runs_dir):
        print(f"‚ùå Eƒüitim dizini bulunamadƒ±: {runs_dir}")
        return False
    
    # Get the latest experiment directory
    exp_dirs = [d for d in os.listdir(runs_dir) if d.startswith('exp')]
    if not exp_dirs:
        print(f"‚ùå Hi√ßbir eƒüitim denemesi bulunamadƒ±: {runs_dir}")
        return False
    
    # Sort to get the latest (exp, exp2, exp3, etc.)
    exp_dirs.sort(key=lambda x: int(x[3:]) if x[3:].isdigit() else 0)
    latest_exp = exp_dirs[-1]
    source_dir = os.path.join(runs_dir, latest_exp, "weights")
    
    if not os.path.exists(source_dir):
        print(f"‚ùå Aƒüƒ±rlƒ±k dizini bulunamadƒ±: {source_dir}")
        return False
    
    # Create target directory
    try:
        os.makedirs(drive_folder_path, exist_ok=True)
        print(f"üìÅ Hedef dizin olu≈üturuldu: {drive_folder_path}")
    except Exception as e:
        print(f"‚ùå Hedef dizin olu≈üturulamadƒ±: {e}")
        return False
    
    # Copy files
    copied_files = []
    
    # Copy best.pt
    if best_file:
        best_path = os.path.join(source_dir, "best.pt")
        if os.path.exists(best_path):
            try:
                target_best = os.path.join(drive_folder_path, "best.pt")
                shutil.copy2(best_path, target_best)
                copied_files.append("best.pt")
                print(f"‚úÖ best.pt kopyalandƒ±: {target_best}")
            except Exception as e:
                print(f"‚ùå best.pt kopyalanamadƒ±: {e}")
        else:
            print(f"‚ö†Ô∏è  best.pt bulunamadƒ±: {best_path}")
    
    # Copy last.pt
    if last_file:
        last_path = os.path.join(source_dir, "last.pt")
        if os.path.exists(last_path):
            try:
                target_last = os.path.join(drive_folder_path, "last.pt")
                shutil.copy2(last_path, target_last)
                copied_files.append("last.pt")
                print(f"‚úÖ last.pt kopyalandƒ±: {target_last}")
            except Exception as e:
                print(f"‚ùå last.pt kopyalanamadƒ±: {e}")
        else:
            print(f"‚ö†Ô∏è  last.pt bulunamadƒ±: {last_path}")
    
    # Copy additional files from project root
    additional_files = ["merged_dataset.yaml", "unified_class_mapping.json", "analysis_report.json"]
    for file_name in additional_files:
        if os.path.exists(file_name):
            try:
                target_file = os.path.join(drive_folder_path, file_name)
                shutil.copy2(file_name, target_file)
                copied_files.append(file_name)
                print(f"‚úÖ {file_name} kopyalandƒ±: {target_file}")
            except Exception as e:
                print(f"‚ùå {file_name} kopyalanamadƒ±: {e}")
    
    # Copy training results and plots if available
    results_dir = os.path.join(runs_dir, latest_exp)
    result_files = ["results.png", "confusion_matrix.png", "F1_curve.png", "P_curve.png", "R_curve.png"]
    for file_name in result_files:
        result_path = os.path.join(results_dir, file_name)
        if os.path.exists(result_path):
            try:
                target_file = os.path.join(drive_folder_path, file_name)
                shutil.copy2(result_path, target_file)
                copied_files.append(file_name)
                print(f"‚úÖ {file_name} kopyalandƒ±: {target_file}")
            except Exception as e:
                print(f"‚ùå {file_name} kopyalanamadƒ±: {e}")
    
    if copied_files:
        print(f"\n‚úÖ Google Drive'a kaydedilen dosyalar: {', '.join(copied_files)}")
        print(f"üìÅ Kaydetme konumu: {drive_folder_path}")
        print(f"üóÇÔ∏è  Toplam kaydedilen dosya: {len(copied_files)}")
        return True
    else:
        print("‚ùå Kopyalanacak dosya bulunamadƒ±.")
        return False

# --- Yardƒ±mcƒ±: master sƒ±nƒ±f isimlerini y√ºkleyip class_ids.json yaz ---
def _load_names_from_yaml(yaml_path: str):
    try:
        if os.path.exists(yaml_path):
            with open(yaml_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f) or {}
            names = []
            if isinstance(data.get('names'), list):
                names = data['names']
            elif isinstance(data.get('names'), dict):
                names = [v for k, v in sorted(((int(k), v) for k, v in data['names'].items()), key=lambda x: x[0])]
            elif isinstance(data.get('classes'), list):
                names = data['classes']
            return [str(x) for x in names] if names else None
    except Exception:
        return None
    return None

def _write_class_ids_json(configs_dir: str) -> bool:
    try:
        # √ñncelik master_data.yaml, yoksa merged_dataset.yaml
        names = _load_names_from_yaml('master_data.yaml') or _load_names_from_yaml('merged_dataset.yaml')
        if not names:
            return False
        payload = {
            'generated_at': datetime.now().isoformat(),
            'names': names,
            'id_to_name': [{'id': i, 'name': n} for i, n in enumerate(names)],
        }
        os.makedirs(configs_dir, exist_ok=True)
        out_path = os.path.join(configs_dir, 'class_ids.json')
        with open(out_path, 'w', encoding='utf-8') as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ class_ids.json yazƒ±ldƒ±: {out_path}")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è class_ids.json yazƒ±lamadƒ±: {e}")
        return False

def download_models_menu():
    """Interactive menu for downloading YOLO11 models"""
    print(f"\n{get_text('model_download_title')}")
    
    if is_colab():
        default_dir = get_smartfarm_models_dir() or "/content/colab_learn/yolo11_models"
    else:
        default_dir = "yolo11_models"
    
    # Global timestamp ile model dizini olu≈ütur
    base_save_dir = input(get_text('save_directory', default=default_dir)) or default_dir
    
    # Global timestamp'i kullanarak alt klas√∂r olu≈ütur
    global_ts = get_global_timestamp()
    save_dir = os.path.join(base_save_dir, global_ts)
    
    print(f"üåê Modeller global timestamp klas√∂r√ºne indirilecek: {save_dir}")
    
    # Klas√∂r√º olu≈ütur
    os.makedirs(save_dir, exist_ok=True)
    
    print(f"\n{get_text('download_options')}")
    print(get_text('single_model'))
    print(get_text('all_detection'))
    print(get_text('all_models'))
    
    choice = input(f"\n{get_text('your_choice')}") or "2"
    
    if choice == "1":
        print(f"\n{get_text('select_model_type')}")
        print(get_text('detection_default'))
        print(get_text('segmentation'))
        print(get_text('classification'))
        print(get_text('pose'))
        print(get_text('obb'))
        
        model_type_map = {
            "1": "detection",
            "2": "segmentation", 
            "3": "classification",
            "4": "pose",
            "5": "obb"
        }
        
        model_type_choice = input(get_text('enter_choice_1_5')) or "1"
        model_type = model_type_map.get(model_type_choice, "detection")
        
        print(f"\n{get_text('select_model_size')}")
        print(get_text('small'))
        print(get_text('medium_default'))
        print(get_text('large'))
        print(get_text('extra_large'))
        
        size_map = {
            "1": "s",
            "2": "m",
            "3": "l",
            "4": "x"
        }
        
        size_choice = input(get_text('enter_choice_1_4')) or "2"
        size = size_map.get(size_choice, "m")
        
        model_path = download_specific_model_type(model_type, size, save_dir)
        if model_path:
            print(f"\n‚úÖ Model ba≈üarƒ±yla indirildi: {model_path}")
            print(f"üìÅ Global timestamp klas√∂r√º: {os.path.basename(save_dir)}")
    
    elif choice == "2":
        detection_models = ["yolo11s.pt", "yolo11m.pt", "yolo11l.pt", "yolo11x.pt"]
        downloaded = download_yolo11_models(save_dir, detection_models)
        print(f"\n‚úÖ {len(downloaded)} tespit modeli indirildi: {save_dir}")
        print(f"üìÅ Global timestamp klas√∂r√º: {os.path.basename(save_dir)}")
    
    elif choice == "3":
        downloaded = download_yolo11_models(save_dir)
        print(f"\n‚úÖ {len(downloaded)} model indirildi: {save_dir}")
        print(f"üìÅ Global timestamp klas√∂r√º: {os.path.basename(save_dir)}")
    
    else:
        print("\n‚ùå Ge√ßersiz se√ßim. Hi√ßbir model indirilmedi.")
        return None
    
    return save_dir

def hierarchical_dataset_setup():
    """Setup for hierarchical multi-dataset training"""
    print("\n===== Hiyerar≈üik √áoklu Veri Seti Kurulumu =====")
    
    # Initialize the YAML-based dataset manager
    config_file = input("Konfig√ºrasyon dosyasƒ± yolu (varsayƒ±lan: config_datasets.yaml): ") or "config_datasets.yaml"
    
    if not os.path.exists(config_file):
        print(f"‚ùå Konfig√ºrasyon dosyasƒ± bulunamadƒ±: {config_file}")
        print("L√ºtfen config_datasets.yaml dosyasƒ±nƒ±n mevcut dizinde olduƒüundan emin olun")
        return None
    
    manager = YAMLBasedMultiDatasetManager(config_file=config_file)
    
    # Opsiyonel: Roboflow API key giri≈üi (bo≈ü bƒ±rakƒ±labilir)
    try:
        print("\nüîë Roboflow API (opsiyonel)")
        entered_key = input("API key girin (bo≈ü ge√ßebilirsiniz): ").strip()
        if entered_key:
            manager.api_key = entered_key
            # API key girildiyse, kullanƒ±cƒ±ya split ayarƒ±nƒ± da soralƒ±m
            split_cfg = get_dataset_split_config(entered_key)
            if split_cfg:
                manager.split_config = split_cfg
        else:
            # Bo≈üsa, varsa config'den otomatik kullanƒ±lacak (indirme sƒ±rasƒ±nda fallback var)
            manager.api_key = None
            manager.split_config = None
    except Exception:
        # Sessiz ge√ß
        pass

    # Show system information
    print(f"\nüìä Sistem Bilgileri:")
    print(f"‚úÖ Konfig√ºrasyon y√ºklendi: {config_file}")
    print(f"üìÅ Mevcut gruplar: {len(manager.get_available_dataset_groups())}")
    
    # Interactive dataset selection
    selected_group = manager.interactive_dataset_selection()
    
    if not selected_group:
        print("‚ùå Hi√ßbir veri seti grubu se√ßilmedi")
        return None
    
    # Get recommendations
    recommendations = manager.get_training_recommendations(selected_group)
    
    print(f"\nüéØ '{selected_group}' i√ßin Eƒüitim √ñnerileri:")
    for key, value in recommendations.items():
        if isinstance(value, list):
            print(f"  {key}:")
            for item in value:
                print(f"    ‚Ä¢ {item}")
        else:
            print(f"  {key}: {value}")
    
    # Get global settings
    settings = manager.get_global_settings()
    
    # Ask for target count per class (global default + opsiyonel sƒ±nƒ±f bazlƒ± hedefler)
    default_target = settings.get('default_target_count_per_class', 5000)
    while True:
        try:
            target_count = int(input(f"\nSƒ±nƒ±f ba≈üƒ±na hedef √∂rnek sayƒ±sƒ± (varsayƒ±lan: {default_target}): ") or str(default_target))
            if target_count > 0:
                break
            print("‚ùå L√ºtfen pozitif bir sayƒ± girin.")
        except ValueError:
            print("‚ùå L√ºtfen ge√ßerli bir sayƒ± girin.")

    # Opsiyonel: Kullanƒ±cƒ± sƒ±nƒ±f bazƒ±nda √∂zel hedef sayƒ±larƒ± girmek isterse
    per_class_targets = None
    customize = (input("\nSƒ±nƒ±f bazƒ±nda hedef sayƒ±larƒ± √∂zelle≈ütirmek ister misiniz? (e/h, varsayƒ±lan: h): ") or "h").lower()
    if customize.startswith('e'):
        per_class_targets = {}
        print("\nSƒ±nƒ±f bazlƒ± hedefler (bo≈ü bƒ±rakƒ±lƒ±rsa genel varsayƒ±lan kullanƒ±lacak):")
        for cls in manager.hierarchical_classes.keys():
            try:
                val = input(f"  ‚Ä¢ {cls} i√ßin hedef (varsayƒ±lan {target_count}): ") or str(target_count)
                if val.strip() == "":
                    continue
                n = int(val)
                if n > 0:
                    per_class_targets[cls] = n
            except Exception:
                pass
    
    # Output directory
    default_output = "datasets/hierarchical_merged"
    output_dir = input(f"\nBirle≈ütirilmi≈ü veri seti dizini (varsayƒ±lan: {default_output}): ") or default_output

    return {
        'manager': manager,
        'selected_group': selected_group,
        'target_count': target_count,
        'per_class_targets': per_class_targets,
        'output_dir': output_dir,
        'recommendations': recommendations,
        'settings': settings
    }

def process_hierarchical_datasets(dataset_config):
    """Process hierarchical multi-datasets"""
    print("\n===== Hiyerar≈üik √áoklu Veri Setleri ƒ∞≈üleniyor =====")
    
    manager = dataset_config['manager']
    target_count = dataset_config['target_count']
    
    try:
        # 1. Download datasets
        print("\n1Ô∏è‚É£ Veri setleri indiriliyor...")
        download_success = manager.download_all_datasets()
        
        if not download_success:
            print("‚ùå Veri seti indirme ba≈üarƒ±sƒ±z!")
            return False

        # Etiket Yeniden E≈üleme Modu ‚Äî ƒ∞NDƒ∞RME SONRASI ta≈üƒ±ndƒ±
        print("\nEtiket Yeniden E≈üleme Modu:")
        print("1) Merge a≈üamasƒ±nda alt-sƒ±nƒ±f etiketleri KORUNMAZ; t√ºm kutular ANA sƒ±nƒ±fa toplanƒ±r (varsayƒ±lan)")
        print("2) Merge a≈üamasƒ±nda alt-sƒ±nƒ±f etiketleri KORUNUR; t√ºm kutular ana sƒ±nƒ±fa toplanmaz")
        while True:
            label_mode_choice = (input("Se√ßenek [1-2] (varsayƒ±lan: 1): ") or "1").strip()
            if label_mode_choice in ["1", "2"]:
                break
            print("‚ùå L√ºtfen 1 veya 2 giriniz.")
        dataset_config['label_mode'] = "collapse_to_main" if label_mode_choice == "1" else "preserve_subclasses"

        # 2. Create unified class mapping
        print("\n2Ô∏è‚É£ Hiyerar≈üik sƒ±nƒ±f haritalamasƒ± olu≈üturuluyor...")
        classes_created = manager.create_unified_class_mapping()
        
        if classes_created == 0:
            print("‚ùå Hi√ßbir sƒ±nƒ±f haritalandƒ±rƒ±lamadƒ±!")
            return False
        
        print(f"‚úÖ {classes_created} ana sƒ±nƒ±f olu≈üturuldu")

        # 3. Label mode y√∂nlendirmesi
        label_mode = dataset_config.get('label_mode') or dataset_config.get('setup', {}).get('label_mode')
        if label_mode == 'preserve_subclasses':
            print("\n‚öôÔ∏è Se√ßenek 2: Alt-sƒ±nƒ±f etiketleri KORUNACAK. Global remap otomatik ba≈ülatƒ±lƒ±yor...")
            try:
                cmd = [
                    sys.executable,
                    str(Path("tools") / "remap_from_all_yaml.py"),
                    "--root", "datasets",
                    "--force-backup",
                ]
                print(f"[APPLY] Remap komutu: {' '.join(cmd)}")
                proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                if proc.stdout:
                    print(proc.stdout)
                if proc.returncode != 0:
                    print(f"[UYARI] Remap sƒ±rasƒ±nda hata olu≈ütu (kod={proc.returncode}). Devam ediliyor...\n{proc.stderr}")
                else:
                    print("‚úÖ Etiket remap i≈ülemi tamamlandƒ±.")
            except Exception as e:
                print(f"[UYARI] Remap √ßaƒürƒ±sƒ± ba≈üarƒ±sƒ±z: {e}. ƒ∞≈üleme devam edilecek.")

        # 4. Merge step: only for option 1 (collapse_to_main)
        if label_mode == 'preserve_subclasses':
            # Kopyalama yapmadan √ßoklu kaynak YAML √ºret
            try:
                def _resolve_split_image_dirs(local_root: str, split: str):
                    """Split i√ßin olasƒ± g√∂r√ºnt√º klas√∂rlerini tespit et."""
                    cand = []
                    data_yaml_path = os.path.join(local_root, 'data.yaml')
                    if os.path.exists(data_yaml_path):
                        try:
                            with open(data_yaml_path, 'r', encoding='utf-8') as f:
                                dcfg = yaml.safe_load(f) or {}
                            base_path = dcfg.get('path') or '.'
                            entry = dcfg.get('train' if split == 'train' else ('val' if 'val' in split else split))
                            # val/valid e≈ü anlamlƒ±
                            if not entry and split == 'val':
                                entry = dcfg.get('valid')
                            if entry:
                                def _join(root_dir, p):
                                    return p if os.path.isabs(p) else os.path.normpath(os.path.join(root_dir, p))
                                full = _join(local_root, _join('' if base_path == '.' else base_path, entry))
                                cand.append(full)
                                # images -> ensure it's images dir
                                if os.path.basename(full) != 'images' and os.path.isdir(full):
                                    # kullanƒ±cƒ± doƒürudan images klas√∂r√ºn√º deƒüil √ºst klas√∂r√º vermi≈ü olabilir
                                    cand.append(os.path.join(full, 'images'))
                        except Exception:
                            pass
                    # Yaygƒ±n dizinler
                    synonyms = [split]
                    if split == 'val':
                        synonyms.append('valid')
                    if split == 'valid':
                        synonyms.append('val')
                    for s in synonyms:
                        cand.extend([
                            os.path.join(local_root, 'images', s),
                            os.path.join(local_root, s, 'images'),
                        ])
                    # Var olanlarƒ± filtrele
                    uniq = []
                    seen = set()
                    for p in cand:
                        if p and os.path.isdir(p) and p not in seen:
                            uniq.append(os.path.abspath(p))
                            seen.add(p)
                    return uniq

                # Label klas√∂rlerini bulmak i√ßin yardƒ±mcƒ±
                def _resolve_split_label_dirs(local_root: str, split: str):
                    cand = []
                    data_yaml_path = os.path.join(local_root, 'data.yaml')
                    if os.path.exists(data_yaml_path):
                        try:
                            with open(data_yaml_path, 'r', encoding='utf-8') as f:
                                dcfg = yaml.safe_load(f) or {}
                            base_path = dcfg.get('path') or '.'
                            entry = dcfg.get('train' if split == 'train' else ('val' if 'val' in split else split))
                            if not entry and split == 'val':
                                entry = dcfg.get('valid')
                            if entry:
                                def _join(root_dir, p):
                                    return p if os.path.isabs(p) else os.path.normpath(os.path.join(root_dir, p))
                                full = _join(local_root, _join('' if base_path == '.' else base_path, entry))
                                if os.path.isdir(full):
                                    # images/... kalƒ±bƒ±ndan labels/... tahmini
                                    cand.append(full.replace(os.sep + 'images' + os.sep, os.sep + 'labels' + os.sep))
                                    cand.append(os.path.join(os.path.dirname(full), 'labels'))
                        except Exception:
                            pass
                    synonyms = [split]
                    if split == 'val':
                        synonyms.append('valid')
                    if split == 'valid':
                        synonyms.append('val')
                    for s in synonyms:
                        cand.extend([
                            os.path.join(local_root, 'labels', s),
                            os.path.join(local_root, s, 'labels'),
                        ])
                    uniq = []
                    seen = set()
                    for p in cand:
                        if p and os.path.isdir(p) and p not in seen:
                            uniq.append(os.path.abspath(p))
                            seen.add(p)
                    return uniq

                # T√ºm veri setlerinden train/val klas√∂rlerini topla
                train_dirs = []
                val_dirs = []
                for ds in manager.datasets:
                    root = ds.get('local_path')
                    if not root or not os.path.isdir(root):
                        continue
                    train_dirs.extend(_resolve_split_image_dirs(root, 'train'))
                    # val bulunamazsa valid dene
                    vds = _resolve_split_image_dirs(root, 'val')
                    if not vds:
                        vds = _resolve_split_image_dirs(root, 'valid')
                    # Hi√ß val yoksa train'i yedek olarak kullanma ‚Äî eƒüitim i√ßin val gerekli, ama yoksa bo≈ü bƒ±rakma yerine train'den k√º√ß√ºk bir altk√ºme olu≈üturmanƒ±zƒ± √∂neririz.
                    val_dirs.extend(vds)

                # names y√ºkle
                def _load_master_names():
                    for p in ['config/master_data.yaml', 'master_data.yaml']:
                        if os.path.exists(p):
                            try:
                                with open(p, 'r', encoding='utf-8') as f:
                                    data = yaml.safe_load(f) or {}
                                names = data.get('names') or data.get('classes')
                                if isinstance(names, list) and names:
                                    return [str(x) for x in names]
                            except Exception:
                                pass
                    # Fallback: indirilen veri setlerinden birlik
                    union = []
                    seen = set()
                    for ds in manager.datasets:
                        for c in ds.get('classes', []) or []:
                            if c not in seen:
                                union.append(c)
                                seen.add(c)
                    return union

                # ƒ∞simleri √∂ncelikle config/class_ids.json'dan al
                names = None
                try:
                    cid_path = os.path.join('config', 'class_ids.json')
                    if os.path.exists(cid_path):
                        with open(cid_path, 'r', encoding='utf-8') as f:
                            cid = json.load(f)
                        if isinstance(cid.get('names'), list) and cid['names']:
                            names = [str(x) for x in cid['names']]
                except Exception:
                    pass
                if not names:
                    names = _load_master_names()
                if not train_dirs:
                    print("‚ùå Hi√ßbir train dizini bulunamadƒ±. YAML olu≈üturulamadƒ±.")
                    return False
                if not val_dirs:
                    print("‚ö†Ô∏è Val/valid dizini bulunamadƒ±. Train dizinleri kullanƒ±lacak (deƒüerlendirme i√ßin √∂nerilmez).")
                    val_dirs = train_dirs[:]

                dataset_yaml = {
                    'path': '.',
                    'train': train_dirs,
                    'val': val_dirs,
                    'nc': len(names),
                    'names': names,
                }
                os.makedirs('config', exist_ok=True)
                merged_yaml_path = os.path.join('config', 'merged_class_dataset.yaml')
                with open(merged_yaml_path, 'w', encoding='utf-8') as f:
                    yaml.dump(dataset_yaml, f, sort_keys=False, allow_unicode=True)
                print("üìÑ config/merged_class_dataset.yaml olu≈üturuldu (kopyasƒ±z √ßoklu kaynak).")
                print(f"  ‚Ä¢ train kaynak sayƒ±sƒ±: {len(train_dirs)}")
                print(f"  ‚Ä¢ val kaynak sayƒ±sƒ±: {len(val_dirs)}")
            except Exception as e:
                print(f"‚ùå merged_dataset.yaml olu≈üturulurken hata: {e}")
                return False
            # Merge kopyalama adƒ±mƒ± atlanƒ±r fakat eƒüitim i√ßin YAML hazƒ±r.
            print("\n‚ÑπÔ∏è Se√ßenek 2 i√ßin kopyalama yapmadan eƒüitim YAML hazƒ±rlandƒ± (config/merged_class_dataset.yaml).")

            # Opsiyonel: Hedefe TAMAMLAMA augmentation (yalnƒ±zca eksik kadar √ºret)
            try:
                if _AUG_PIPE_AVAILABLE:
                    # e/h girdisi i√ßin doƒürulama d√∂ng√ºs√º
                    while True:
                        raw = input("\nSƒ±nƒ±f ba≈üƒ±na hedefe TAMAMLAMA i√ßin augmentation uygulansƒ±n mƒ±? (e/h, varsayƒ±lan: h): ")
                        resp = (raw or 'h').strip().lower()
                        if resp in ('e', 'h'):
                            break
                        print("Ge√ßersiz giri≈ü. L√ºtfen 'e' veya 'h' girin.")
                    if resp == 'e':
                        # Hedef belirleme: sayƒ±sal doƒürulama d√∂ng√ºs√º
                        target_default = dataset_config.get('target_count')
                        try:
                            target_default = int(target_default)
                        except Exception:
                            target_default = 2000
                        while True:
                            raw_t = input(f"Hedef sƒ±nƒ±f ba≈üƒ±na √∂rnek (bo≈ü bƒ±rak: {target_default}): ")
                            if not raw_t or not raw_t.strip():
                                target_numeric = target_default
                                break
                            try:
                                target_numeric = int(raw_t.strip())
                                if target_numeric <= 0:
                                    print("Ge√ßersiz deƒüer. Pozitif bir tam sayƒ± girin.")
                                    continue
                                break
                            except ValueError:
                                print("Ge√ßersiz deƒüer. L√ºtfen bir tam sayƒ± girin.")

                        # Train label ve image dosyalarƒ±nƒ± topla
                        image_dirs = train_dirs[:]
                        label_dirs = []
                        for ds in manager.datasets:
                            root = ds.get('local_path')
                            if not root or not os.path.isdir(root):
                                continue
                            label_dirs.extend(_resolve_split_label_dirs(root, 'train'))

                        def _iter_label_files(dirs):
                            for d in dirs:
                                for root_dir, _, files in os.walk(d):
                                    for fn in files:
                                        if fn.lower().endswith('.txt'):
                                            yield os.path.join(root_dir, fn)

                        def _match_image_for_label(label_path, img_dirs):
                            base = os.path.splitext(os.path.basename(label_path))[0]
                            for d in img_dirs:
                                for ext in ['.jpg', '.jpeg', '.png']:
                                    cand = os.path.join(d, base + ext)
                                    if os.path.exists(cand):
                                        return cand
                            return None

                        all_label_paths = []
                        all_image_paths = []
                        for lp in _iter_label_files(label_dirs):
                            ip = _match_image_for_label(lp, image_dirs)
                            if ip:
                                all_label_paths.append(lp)
                                all_image_paths.append(ip)

                        if not all_label_paths:
                            print("‚ö†Ô∏è  Augmentation i√ßin train etiket dosyasƒ± bulunamadƒ±. Adƒ±m atlandƒ±.")
                        else:
                            out_dir = os.path.join('datasets', 'balanced_aug')
                            os.makedirs(out_dir, exist_ok=True)
                            print(f"\n‚öôÔ∏è  Hedefe-tamamlama ba≈ülƒ±yor. Hedef: {target_numeric}  √áƒ±kƒ±≈ü: {out_dir}")
                            pipe = YOLOAugmentationPipeline(image_size=dataset_config.get('settings', {}).get('default_image_size', 640))
                            
                            # Resolve VAL/TEST image/label directories from all datasets
                            val_img_dirs = []
                            val_lbl_dirs = []
                            test_img_dirs = []
                            test_lbl_dirs = []
                            for ds in manager.datasets:
                                root = ds.get('local_path')
                                if not root or not os.path.isdir(root):
                                    continue
                                # val/valid images
                                vids = _resolve_split_image_dirs(root, 'val')
                                if not vids:
                                    vids = _resolve_split_image_dirs(root, 'valid')
                                val_img_dirs.extend(vids)
                                # val/valid labels
                                vlds = _resolve_split_label_dirs(root, 'val')
                                if not vlds:
                                    vlds = _resolve_split_label_dirs(root, 'valid')
                                val_lbl_dirs.extend(vlds)
                                # test images/labels
                                test_img_dirs.extend(_resolve_split_image_dirs(root, 'test'))
                                test_lbl_dirs.extend(_resolve_split_label_dirs(root, 'test'))

                            # Decide automatically (no prompt): copy val/test based on config defaults
                            default_copy_val_test = None
                            try:
                                default_copy_val_test = dataset_config.get('settings', {}).get('copy_val_test_default')
                                if default_copy_val_test is not None:
                                    default_copy_val_test = bool(default_copy_val_test)
                            except Exception:
                                default_copy_val_test = None
                            if default_copy_val_test is None:
                                try:
                                    with open(os.path.join('config', 'master_data.yaml'), 'r', encoding='utf-8') as f:
                                        _md = yaml.safe_load(f) or {}
                                    default_copy_val_test = bool(_md.get('copy_val_test_default', True))
                                except Exception:
                                    default_copy_val_test = True
                            copy_val_test = bool(default_copy_val_test)

                            pipe.augment_dataset_batch(
                                all_image_paths,
                                all_label_paths,
                                out_dir,
                                target_numeric,
                                copy_val_test=copy_val_test,
                                val_image_dirs=val_img_dirs,
                                val_label_dirs=val_lbl_dirs,
                                test_image_dirs=test_img_dirs,
                                test_label_dirs=test_lbl_dirs,
                            )

                            # Create config/augmented_train.yaml to reference augmented train and original or copied val/test
                            try:
                                # Determine YAML val/test paths
                                if copy_val_test:
                                    yaml_val = os.path.join(out_dir, 'val', 'images')
                                    yaml_test = os.path.join(out_dir, 'test', 'images') if test_img_dirs or test_lbl_dirs else None
                                else:
                                    # Use original directories (list is supported)
                                    yaml_val = val_img_dirs if val_img_dirs else None
                                    yaml_test = test_img_dirs if test_img_dirs else None

                                if not names:
                                    # Try to reload names from config/class_ids.json as fallback
                                    try:
                                        with open(os.path.join('config', 'class_ids.json'), 'r', encoding='utf-8') as f:
                                            cid = json.load(f)
                                        if isinstance(cid.get('names'), list) and cid['names']:
                                            local_names = [str(x) for x in cid['names']]
                                        else:
                                            local_names = []
                                    except Exception:
                                        local_names = []
                                else:
                                    local_names = names

                                yaml_payload = {
                                    'path': '.',
                                    'train': os.path.join(out_dir, 'train', 'images'),
                                    'val': yaml_val if yaml_val is not None else [],
                                    'test': yaml_test if yaml_test is not None else [],
                                    'names': local_names,
                                }
                                os.makedirs('config', exist_ok=True)
                                aug_yaml_path = os.path.join('config', 'augmented_train.yaml')
                                with open(aug_yaml_path, 'w', encoding='utf-8') as f:
                                    yaml.dump(yaml_payload, f, sort_keys=False, allow_unicode=True)
                                print(f"\nüìÑ Eƒüitim YAML yazƒ±ldƒ±: {aug_yaml_path}")
                                print(f"  ‚Ä¢ train: {yaml_payload['train']}")
                                print(f"  ‚Ä¢ val:   {yaml_payload['val'] if yaml_payload['val'] else '‚Äî'}")
                                print(f"  ‚Ä¢ test:  {yaml_payload['test'] if yaml_payload['test'] else '‚Äî'}")

                                # Optional: Ask user to enable time-based Drive copy for training (default: disabled)
                                try:
                                    cfg_path = 'config_datasets.yaml'
                                    def_minutes = 30
                                    if os.path.exists(cfg_path):
                                        with open(cfg_path, 'r', encoding='utf-8') as _cf:
                                            _cfg = yaml.safe_load(_cf) or {}
                                        gs = _cfg.get('global_settings', {}) if isinstance(_cfg, dict) else {}
                                        def_minutes = int(gs.get('time_based_copy_interval_minutes', 30))
                                    yn = (input("\nS√ºreye baƒülƒ± Drive kopyalama a√ßƒ±lsƒ±n mƒ±? (e/h, varsayƒ±lan: h): ") or 'h').strip().lower()
                                    use_time_based = yn.startswith('e')
                                    minutes = def_minutes
                                    if use_time_based:
                                        min_in = input(f"Kopyalama aralƒ±ƒüƒ± (dakika, varsayƒ±lan: {def_minutes}): ").strip()
                                        minutes = int(min_in) if min_in else def_minutes
                                    # Persist selection into config_datasets.yaml
                                    try:
                                        _cfg = {}
                                        if os.path.exists(cfg_path):
                                            with open(cfg_path, 'r', encoding='utf-8') as _cf:
                                                _cfg = yaml.safe_load(_cf) or {}
                                        if not isinstance(_cfg, dict):
                                            _cfg = {}
                                        _gs = _cfg.get('global_settings')
                                        if not isinstance(_gs, dict):
                                            _gs = {}
                                        _gs['use_time_based_copy_default'] = bool(use_time_based)
                                        _gs['time_based_copy_interval_minutes'] = int(minutes)
                                        _cfg['global_settings'] = _gs
                                        with open(cfg_path, 'w', encoding='utf-8') as _cf:
                                            yaml.dump(_cfg, _cf, sort_keys=False, allow_unicode=True)
                                        state_txt = 'A√áIK' if use_time_based else 'KAPALI'
                                        print(f"‚úÖ S√ºreye baƒülƒ± kopyalama ayarƒ± g√ºncellendi: {state_txt}, {minutes} dk")
                                        # Ayrƒ±ca kullanƒ±cƒ± se√ßimini configs/ klas√∂r√ºne de kaydet
                                        try:
                                            os.makedirs('configs', exist_ok=True)
                                            from datetime import datetime as _dt
                                            ts = _dt.now().strftime('%Y%m%d_%H%M%S')
                                            snapshot = {
                                                'timestamp': ts,
                                                'use_time_based_copy_default': bool(use_time_based),
                                                'time_based_copy_interval_minutes': int(minutes),
                                                'augmented_train_yaml': aug_yaml_path,
                                            }
                                            snap_path = os.path.join('configs', f'time_copy_selection_{ts}.yaml')
                                            with open(snap_path, 'w', encoding='utf-8') as _sf:
                                                yaml.dump(snapshot, _sf, sort_keys=False, allow_unicode=True)
                                            print(f"üìù Se√ßim kopyasƒ± kaydedildi: {snap_path}")
                                        except Exception as _snap_e:
                                            print(f"‚ö†Ô∏è Se√ßim kopyasƒ± kaydedilemedi: {_snap_e}")
                                    except Exception as _werr:
                                        print(f"‚ö†Ô∏è S√ºreye baƒülƒ± kopyalama ayarƒ± kaydedilemedi: {_werr}")
                                except Exception as _tbc_err:
                                    print(f"‚ö†Ô∏è S√ºreye baƒülƒ± kopyalama ayarƒ± sorulamadƒ±: {_tbc_err}")
                            except Exception as _yaml_err:
                                print(f"‚ö†Ô∏è augmented_train.yaml olu≈üturulamadƒ±: {_yaml_err}")
                            print("‚úÖ Hedefe-tamamlama augmentation tamamlandƒ±.")
                else:
                    print("‚ö†Ô∏è  YOLOAugmentationPipeline mevcut deƒüil. Hedefe-tamamlama atlandƒ±.")
            except Exception as e:
                print(f"[UYARI] Hedefe-tamamlama sƒ±rasƒ±nda hata: {e}")

            return True
        
        print("\n3Ô∏è‚É£ Veri setleri hiyerar≈üik yapƒ±yla birle≈ütiriliyor...")
        # Fonksiyona 'setup' dict'i ge√ßirildiƒüi i√ßin doƒürudan buradan oku
        pct = dataset_config.get('per_class_targets')
        target_arg = pct if pct else target_count
        merged_counts = manager.merge_datasets(target_count_per_class=target_arg)
        
        if not merged_counts:
            print("‚ùå Veri seti birle≈ütirme ba≈üarƒ±sƒ±z!")
            return False
        
        print(f"\n‚úÖ Hiyerar≈üik √ßoklu veri seti i≈üleme tamamlandƒ±!")
        print(f"üìÅ Birle≈ütirilmi≈ü veri seti: {manager.output_dir}")
        print(f"üìÑ YAML dosyasƒ±: merged_dataset.yaml")
        print(f"üè∑Ô∏è  Sƒ±nƒ±f haritasƒ±: unified_class_mapping.json")
        
        # Display final statistics
        total_samples = sum(merged_counts.values())
        print(f"\nüìä Son Veri Seti ƒ∞statistikleri:")
        print(f"   Toplam √∂rnek: {total_samples:,}")
        print(f"   Ana sƒ±nƒ±flar: {len(merged_counts)}")
        print(f"   Sƒ±nƒ±f ba≈üƒ±na √∂rnek: {total_samples // len(merged_counts):,} (ortalama)")
        
        for class_name, count in merged_counts.items():
            print(f"   {class_name}: {count:,}")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Hiyerar≈üik veri seti i≈üleme sƒ±rasƒ±nda hata: {e}")
        import traceback
        traceback.print_exc()
        return False

def interactive_training_setup():
    """Interactive training parameter setup for hierarchical model"""
    print("\n===== Hiyerar≈üik Model Eƒüitim Kurulumu =====")
    
    # Dataset type selection
    print("\nVeri seti konfig√ºrasyonu:")
    print("1) Hiyerar≈üik √ßoklu veri seti (√ñnerilen)")
    print("2) Tek Roboflow veri seti (Eski)")
    
    while True:
        dataset_choice = input("\nSe√ßenek [1-2] (varsayƒ±lan: 1): ") or "1"
        if dataset_choice in ["1", "2"]:
            break
        print("‚ùå L√ºtfen 1 veya 2 se√ßin.")
    
    dataset_config = {}
    
    if dataset_choice == "1":
        # Hierarchical multi-dataset
        dataset_setup = hierarchical_dataset_setup()
        if not dataset_setup:
            return None
        
        dataset_config = {
            'type': 'hierarchical_multi',
            'setup': dataset_setup,
            'data_yaml': 'merged_dataset.yaml'
        }
    else:
        # Single dataset (legacy) - Roboflow API y√∂netimi ile
        roboflow_url = input("\nRoboflow URL (varsayƒ±lan: bo≈ü): ").strip() or ""
        if not roboflow_url:
            print("‚ùå URL saƒülanmadƒ±")
            return None
        
        # Roboflow API key y√∂netimi
        api_result = handle_roboflow_api_management(roboflow_url)
        
        dataset_config = {
            'type': 'single',
            'url': roboflow_url,
            'api_key': api_result['api_key'],
            'split_config': api_result['split_config'],
            'data_yaml': 'dataset.yaml'
        }
    
    # Project category
    print("\nProje kategorisi:")
    print("1) Hiyerar≈üik Tarƒ±msal AI (√ñnerilen)")
    print("2) Hastalƒ±k Tespiti")
    print("3) Zararlƒ± Tespiti")
    print("4) Karma Tarƒ±msal")
    print("5) √ñzel")
    
    while True:
        category_choice = input("\nKategori se√ßin [1-5] (varsayƒ±lan: 1): ") or "1"
        category_options = {
            "1": "hierarchical_agricultural",
            "2": "diseases",
            "3": "pests", 
            "4": "mixed",
            "5": "custom"
        }
        
        if category_choice in category_options:
            category = category_options[category_choice]
            if category == "custom":
                category = input("√ñzel kategori adƒ± girin: ").strip() or "custom"
            break
        print("‚ùå L√ºtfen 1-5 arasƒ± se√ßin.")
    
    # Get training recommendations
    if dataset_config['type'] == 'hierarchical_multi':
        recommendations = dataset_config['setup']['recommendations']
        recommended_model = recommendations.get('model', 'yolo11l.pt')
        recommended_batch = recommendations.get('batch_size', 8)
        recommended_size = recommendations.get('image_size', 640)
        estimated_time = recommendations.get('estimated_time', 'Unknown')
        
        print(f"\nüéØ Hiyerar≈üik model i√ßin √∂neriler:")
        print(f"   Model: {recommended_model}")
        print(f"   Batch boyutu: {recommended_batch}")
        print(f"   G√∂r√ºnt√º boyutu: {recommended_size}")
        print(f"   Tahmini s√ºre: {estimated_time}")
        
        # Show special notes if available
        special_notes = recommendations.get('special_notes', [])
        if special_notes:
            print(f"   √ñzel hususlar:")
            for note in special_notes:
                print(f"     ‚Ä¢ {note}")
    
    # Google Drive save settings (daha erken sorulsun)
    drive_save_path = None
    if is_colab():
        print("\nGoogle Drive kaydetme ayarlarƒ±:")
        save_to_drive_opt = input("Eƒüitim sonu√ßlarƒ±nƒ± Google Drive'a kaydet? (e/h, varsayƒ±lan: e): ").lower() or "e"
        
        if save_to_drive_opt.startswith("e"):
            default_drive_path = get_smartfarm_models_dir() or "/content/drive/MyDrive/SmartFarm/colab_learn/yolo11_models"
            base_input = input(f"Kaydetme dizini (varsayƒ±lan: {default_drive_path}): ") or default_drive_path

            # Global timestamp'i kullan
            timestamp_dir = None
            try:
                # 1) Global timestamp varsa onu kullan
                global_ts = get_global_timestamp()
                if global_ts:
                    timestamp_dir = os.path.join(base_input, global_ts)
                    print(f"üåê Global timestamp kullanƒ±lƒ±yor: {global_ts}")
                else:
                    # 2) Fallback: DriveManager'da aktif timestamp var mƒ±?
                    if _DRIVE_AVAILABLE:
                        dm_probe = DriveManager()
                        if dm_probe.authenticate():
                            try:
                                if hasattr(dm_probe, 'load_drive_config'):
                                    dm_probe.load_drive_config()
                            except Exception:
                                pass
                            ts_existing = dm_probe.get_timestamp_dir()
                            if ts_existing and os.path.basename(os.path.dirname(ts_existing)) == 'yolo11_models':
                                timestamp_dir = ts_existing
                    # 3) Base klas√∂rde mevcut timestamp dizinlerini tara ve ILK OLU≈ûANINI al (ilk timestamp kuralƒ±)
                    if not timestamp_dir and os.path.isdir(base_input):
                        candidates = [
                            os.path.join(base_input, d)
                            for d in os.listdir(base_input)
                            if os.path.isdir(os.path.join(base_input, d)) and TIMESTAMP_PATTERN.match(d)
                        ]
                        if candidates:
                            # mtime'a g√∂re artan sƒ±rala: ilk eleman en eski (ilk olu≈üturulan)
                            candidates.sort(key=lambda p: os.path.getmtime(p))
                            timestamp_dir = candidates[0]
                            print(f"üïí ƒ∞lk timestamp kuralƒ±: mevcutlardan EN ESKƒ∞Sƒ∞ kullanƒ±lacak ‚Üí {os.path.basename(timestamp_dir)}")
                    # 4) Hi√ßbiri yoksa global timestamp ile yeni olu≈ütur
                    if not timestamp_dir:
                        timestamp = get_global_timestamp()
                        timestamp_dir = os.path.join(base_input, timestamp)
            except Exception:
                # Son √ßare: yeni timestamp olu≈ütur
                timestamp = get_global_timestamp()
                timestamp_dir = os.path.join(base_input, timestamp)
            checkpoints_dir = os.path.join(timestamp_dir, 'checkpoints')
            models_dir = os.path.join(timestamp_dir, 'models')
            logs_dir = os.path.join(timestamp_dir, 'logs')
            configs_dir = os.path.join(timestamp_dir, 'configs')

            try:
                created_any = False
                for d in [checkpoints_dir, models_dir, logs_dir, configs_dir]:
                    if not os.path.isdir(d):
                        os.makedirs(d, exist_ok=True)
                        created_any = True
                action = "kullanƒ±lƒ±yor" if not created_any else "hazƒ±rlandƒ±"
                print(f"‚úÖ Drive timestamp {action}: {timestamp_dir}")
                print(f"üóÇÔ∏è  Kayƒ±t hedefi (checkpoints): {checkpoints_dir}")
                # Eƒüitim opsiyonlarƒ±nda doƒürudan 'checkpoints' klas√∂r√ºn√º hedefle
                drive_save_path = checkpoints_dir
                # Etiket modu 2 ise: sƒ±nƒ±f ID listesini configs/ altƒ±na yaz (varsa)
                try:
                    if (dataset_config.get('type') == 'hierarchical_multi' and
                        (dataset_config.get('setup') or {}).get('label_mode') == 'preserve_subclasses'):
                        _write_class_ids_json(configs_dir)
                except Exception:
                    pass
            except Exception as e:
                print(f"‚ùå Drive klas√∂rleri olu≈üturulamadƒ±: {e}")
                drive_save_path = None
    
    # Eƒüitim parametreleri (g√∂r√ºnt√º boyutu -> batch -> epoch)
    # Model size selection (tek soru)
    print("\nModel boyutunu se√ßin:")
    print("1) yolo11s.pt - K√º√ß√ºk (en hƒ±zlƒ±, d√º≈ü√ºk doƒüruluk)")
    print("2) yolo11m.pt - Orta (dengeli)")
    print("3) yolo11l.pt - B√ºy√ºk (y√ºksek doƒüruluk, yava≈ü) [Hiyerar≈üik i√ßin √∂nerilen]")
    print("4) yolo11x.pt - √áok B√ºy√ºk (en y√ºksek doƒüruluk, en yava≈ü)")

    while True:
        model_choice = input("\nModel se√ßin [1-4] (varsayƒ±lan: 3): ") or "3"
        
        model_options = {
            "1": "yolo11s.pt",
            "2": "yolo11m.pt",
            "3": "yolo11l.pt",
            "4": "yolo11x.pt"
        }
        
        if model_choice in model_options:
            model = model_options[model_choice]
            
            # Check if model exists locally/Drive
            if is_colab():
                model_dir = get_smartfarm_models_dir() or os.path.join("/content/colab_learn", "yolo11_models")
            else:
                model_dir = "yolo11_models"
            model_path = os.path.join(model_dir, model)
            
            if not os.path.exists(model_path):
                print(f"\n‚ö†Ô∏è  Model {model} yerel olarak bulunamadƒ±.")
                download_now = input("≈ûimdi indir? (e/h, varsayƒ±lan: e): ").lower() or "e"
                
                if download_now.startswith("e"):
                    os.makedirs(model_dir, exist_ok=True)
                    download_specific_model_type("detection", model[6], model_dir)
                else:
                    print(f"‚ÑπÔ∏è  Model eƒüitim sƒ±rasƒ±nda otomatik olarak indirilecek.")
            break
        print("‚ùå L√ºtfen 1-4 arasƒ± se√ßin.")
    
    # Batch size ve image size varsayƒ±lanlarƒ± (Colab i√ßin optimize)
    # √ñneri: batch_size=16, img_size=512 (RAM ve hƒ±z dengesi)
    default_batch = 16
    default_img_size = 512

    # √ñnce g√∂r√ºnt√º boyutu
    while True:
        try:
            img_size = int(input(f"\nG√∂r√ºnt√º boyutu (varsayƒ±lan: {default_img_size}, 32'nin katƒ± olmalƒ± ‚Ä¢ Colab i√ßin 512 √∂nerilir): ") or str(default_img_size))
            if img_size > 0 and img_size % 32 == 0:
                break
            print("‚ùå L√ºtfen 32'nin katƒ± olan pozitif bir sayƒ± girin.")
        except ValueError:
            print("‚ùå L√ºtfen ge√ßerli bir sayƒ± girin.")

    # Sonra batch boyutu
    while True:
        try:
            batch_size = int(input(f"\nBatch boyutu (varsayƒ±lan: {default_batch}, d√º≈ü√ºk RAM i√ßin k√º√ß√ºk): ") or str(default_batch))
            if batch_size > 0:
                break
            print("‚ùå L√ºtfen pozitif bir sayƒ± girin.")
        except ValueError:
            print("‚ùå L√ºtfen ge√ßerli bir sayƒ± girin.")

    # En son epoch
    while True:
        try:
            default_epochs = 1000
            epochs = int(input(f"\nEpoch sayƒ±sƒ± [100-2000 √∂nerilen] (varsayƒ±lan: {default_epochs}): ") or str(default_epochs))
            if epochs > 0:
                break
            print("‚ùå L√ºtfen pozitif bir sayƒ± girin.")
        except ValueError:
            print("‚ùå L√ºtfen ge√ßerli bir sayƒ± girin.")

    # Speed mode (optimize epoch time)
    speed_mode_input = (input("\nHƒ±z modu (cache=ram, workers=8, plots=False) a√ßƒ±lsƒ±n mƒ±? (e/h, varsayƒ±lan: e): ") or "e").lower()
    speed_mode = speed_mode_input.startswith('e')
    
    # Hyperparameter file
    use_hyp = input("\nHiperparametre dosyasƒ± kullan (hyp.yaml)? (e/h, varsayƒ±lan: e): ").lower() or "e"
    use_hyp = use_hyp.startswith("e")
    
    # Device detection
    device = check_gpu()
    
    # Build options dictionary
    options = {
        'dataset_config': dataset_config,
        'epochs': epochs,
        'model': model,
        'batch': batch_size,
        'imgsz': img_size,
        'device': device,
        'workers': None,
        'data': dataset_config['data_yaml'],
        'project': 'runs/train',
        'name': 'exp',
        'pretrained': True,
        'optimizer': 'auto',
        'verbose': True,
        'exist_ok': True,
        'use_hyp': use_hyp,
        'category': category,
        'drive_save_path': drive_save_path,
        'speed_mode': speed_mode
    }
    # --- Veri indirme ba≈ülamadan √ñNCE: Otomatik checkpoint aramasƒ± ve kullanƒ±cƒ±ya bilgi vererek se√ßim alma ---
    try:
        if is_colab() and _DRIVE_AVAILABLE:
            dm = DriveManager()
            if dm.authenticate():
                # Konfig√ºrasyon varsa y√ºkle, yoksa yine de arama yap (projeyi bilmeden de base dizinde arƒ±yor)
                try:
                    if hasattr(dm, 'load_drive_config'):
                        dm.load_drive_config()
                except Exception:
                    pass
                print("\nüîç Drive'da mevcut checkpoint aranƒ±yor (en yeni timestamp'tan geriye doƒüru)...")
                ckpt_path, ckpt_name = dm.find_latest_checkpoint()
                if ckpt_path:
                    print(f"‚úÖ Bulundu: {ckpt_name}\nüìÑ Yol: {ckpt_path}")
                    ask = (input("Kaldƒ±ƒüƒ± yerden devam edilsin mi? (e/h, varsayƒ±lan: e): ") or "e").lower()
                    if ask.startswith('e'):
                        options['resume'] = True
                        options['checkpoint_path'] = ckpt_path
                        print("üîÑ Eƒüitim, veri indirme adƒ±mƒ± atlanarak checkpoint'ten devam edecek.")
                    else:
                        print("‚ÑπÔ∏è Resume iptal edildi. Yeni eƒüitim kurulumu ile devam edilecek.")
                else:
                    print("‚ÑπÔ∏è Drive'da kullanƒ±labilir checkpoint bulunamadƒ±. Yeni eƒüitim kurulumu ile devam edilecek.")
            else:
                print("‚ö†Ô∏è Drive mount/kimlik doƒürulama ba≈üarƒ±sƒ±z. Resume aramasƒ± yapƒ±lamadƒ±.")
        else:
            if not is_colab():
                print("‚ÑπÔ∏è Colab ortamƒ± deƒüil. Drive tabanlƒ± otomatik resume aramasƒ± atlandƒ±.")
            elif not _DRIVE_AVAILABLE:
                print("‚ö†Ô∏è drive_manager i√ße aktarƒ±lamadƒ±. Drive tabanlƒ± otomatik resume aramasƒ± yapƒ±lamadƒ±.")
    except Exception as pre_resume_err:
        print(f"‚ö†Ô∏è Otomatik resume kontrol√º sƒ±rasƒ±nda hata: {pre_resume_err}")

    
    # Display selected parameters
    print("\n===== Se√ßilen Eƒüitim Parametreleri =====")
    print(f"Veri seti tipi: {dataset_config['type']}")
    if dataset_config['type'] == 'hierarchical_multi':
        setup = dataset_config['setup']
        print(f"Veri seti grubu: {setup['selected_group']}")
        print(f"Sƒ±nƒ±f ba≈üƒ±na hedef √∂rnek: {setup['target_count']:,}")
        print(f"√áƒ±ktƒ± dizini: {setup['output_dir']}")
    
    print(f"Model: {model}")
    print(f"Epoch: {epochs}")
    print(f"Batch boyutu: {batch_size}")
    print(f"G√∂r√ºnt√º boyutu: {img_size}")
    print(f"Cihaz: {device}")
    print(f"DataLoader workers: {options['workers']} (hafƒ±za i√ßin d√º≈ü√ºk)")
    print(f"Dataset cache varsayƒ±lanƒ±: {'ram' if speed_mode else 'disk'}")
    print(f"cuDNN benchmark: Enabled (training.py i√ßinde)")
    print(f"Hƒ±z modu: {'A√ßƒ±k' if speed_mode else 'Kapalƒ±'}")
    print(f"Kategori: {category}")
    if dataset_config['type'] == 'hierarchical_multi':
        pct = dataset_config['setup'].get('per_class_targets')
        if pct:
            print("Sƒ±nƒ±f bazlƒ± hedefler: (√∂zet)")
            shown = 0
            for k, v in pct.items():
                print(f"  ‚Ä¢ {k}: {v}")
                shown += 1
                if shown >= 10:
                    print("  ‚Ä¢ ... (daha fazla sƒ±nƒ±f var)")
                    break
    # Kaydetme aralƒ±ƒüƒ± sorusu training.py i√ßinde (men√ºl√º) y√∂netiliyor
    
    if drive_save_path:
        print(f"Drive kaydetme yolu: {drive_save_path}")
        # Kaydedilecek dosyalarƒ± net belirt (checkpoints altƒ±nda)
        print(f"Kaydedilecek dosyalar:")
        print(f"  ‚Ä¢ best.pt  ‚Üí {os.path.join(drive_save_path, 'best.pt')}")
        print(f"  ‚Ä¢ last.pt  ‚Üí {os.path.join(drive_save_path, 'last.pt')}")
    
    confirm = (input("\nBu parametrelerle devam et? (e/h, varsayƒ±lan: e): ") or "e").lower()
    if confirm != 'e' and confirm != 'evet' and confirm != 'yes':
        print("‚ùå Kurulum iptal edildi.")
        return None
    
    return options

def get_dataset_split_config(api_key):
    """API key varsa train/test/val deƒüerlerini al"""
    if not api_key:
        return None
    
    print("\nüìä Dataset B√∂l√ºmleme Ayarlarƒ±")
    print("=" * 40)
    print("API key mevcut - dataset b√∂l√ºmleme ayarlarƒ±nƒ± yapƒ±landƒ±rabilirsiniz")
    
    use_custom_split = input("\n√ñzel train/test/val oranƒ± kullanmak istiyor musunuz? (e/h, varsayƒ±lan: h): ").lower() or "h"
    
    if not use_custom_split.startswith('e'):
        print("‚úÖ Varsayƒ±lan b√∂l√ºmleme kullanƒ±lacak")
        return None
    
    print("\nüìã B√∂l√ºmleme Oranƒ± Giri≈üi:")
    print("Not: Toplam 100 olmalƒ± (train + test + val = 100)")
    
    while True:
        try:
            train_pct = int(input("Train oranƒ± (varsayƒ±lan: 70): ") or "70")
            test_pct = int(input("Test oranƒ± (varsayƒ±lan: 20): ") or "20")
            val_pct = int(input("Validation oranƒ± (varsayƒ±lan: 10): ") or "10")
            
            total = train_pct + test_pct + val_pct
            if total != 100:
                print(f"‚ùå Toplam {total}%. L√ºtfen toplamƒ± 100 yapacak ≈üekilde girin.")
                continue
            
            if train_pct < 50:
                print("‚ö†Ô∏è Train oranƒ± %50'den az. Devam etmek istiyor musunuz? (e/h): ", end="")
                if not input().lower().startswith('e'):
                    continue
            
            split_config = {
                'train': train_pct,
                'test': test_pct, 
                'val': val_pct
            }
            
            print(f"\n‚úÖ B√∂l√ºmleme ayarlarƒ±: Train %{train_pct}, Test %{test_pct}, Val %{val_pct}")
            return split_config
            
        except ValueError:
            print("‚ùå L√ºtfen ge√ßerli sayƒ±lar girin")
            continue

def handle_roboflow_api_management(url):
    """Roboflow API key y√∂netimini handle et"""
    print("\nüîë Roboflow API Y√∂netimi")
    print("=" * 40)
    
    # Mevcut API key kontrol et
    existing_key = get_api_key_from_config()
    if existing_key:
        print(f"‚úÖ Mevcut API key bulundu: {existing_key[:10]}...")
        use_existing = input("Mevcut API key'i kullanmak istiyor musunuz? (e/h, varsayƒ±lan: e): ").lower() or "e"
        if use_existing.startswith('e'):
            # API key varsa split config al
            split_config = get_dataset_split_config(existing_key)
            return {'api_key': existing_key, 'split_config': split_config}
    
    print("\nüìã Se√ßenekler:")
    print("1) API Key gir (train/test/val ayarlarƒ± ile)")
    print("2) API Key olmadan devam et (public dataset)")
    
    while True:
        choice = input("\nSe√ßenek [1-2] (varsayƒ±lan: 2): ").strip() or "2"
        
        if choice == "2":
            print("‚úÖ API key olmadan devam ediliyor (public dataset olarak)")
            return {'api_key': None, 'split_config': None}
        
        elif choice == "1":
            print("\nüìã API Key alma adƒ±mlarƒ±:")
            print("1. https://roboflow.com adresine gidin")
            print("2. Hesabƒ±nƒ±za giri≈ü yapƒ±n")
            print("3. Settings > API sayfasƒ±na gidin")
            print("4. Private API Key'inizi kopyalayƒ±n")
            
            api_key = input("\nüîë API Key'inizi girin (bo≈ü bƒ±rakabilirsiniz): ").strip()
            
            if api_key:
                # API key'i kaydet
                result = handle_roboflow_action('1', api_key=api_key)
                if result:
                    print("‚úÖ API key ba≈üarƒ±yla kaydedildi!")
                    # Split config al
                    split_config = get_dataset_split_config(api_key)
                    return {'api_key': api_key, 'split_config': split_config}
                else:
                    print("‚ùå API key kaydedilemedi, bo≈ü olarak devam ediliyor")
                    return {'api_key': None, 'split_config': None}
            else:
                print("‚úÖ API key bo≈ü bƒ±rakƒ±ldƒ±, public dataset olarak devam ediliyor")
                return {'api_key': None, 'split_config': None}
        
        else:
            print("‚ùå Ge√ßersiz se√ßenek")
            continue

def main():
    """Main function - Hierarchical Multi-Dataset Training Framework"""
    # Language selection at startup
    language_choice = select_language()
    
    # --- Dil se√ßiminden hemen sonra: Global timestamp sabitleme sistemi ---
    try:
        # Get language choice for prompts
        lang = 'tr' if get_text('language_choice', default='tr').startswith('tr') else 'en'
        
        # Ask user for timestamp choice and set global timestamp
        global_timestamp = ask_user_for_timestamp_choice(lang)
        
        # Set environment variable for other processes
        os.environ['SMARTFARM_GLOBAL_TIMESTAMP'] = global_timestamp
        
        if lang == 'tr':
            print(f"üåê Global timestamp oturumu: {global_timestamp}")
            print(f"üìÅ Bu timestamp t√ºm i≈ülemlerde (Google Drive, model indirme, eƒüitim) tutarlƒ± kullanƒ±lacak")
        else:
            print(f"üåê Global timestamp session: {global_timestamp}")
            print(f"üìÅ This timestamp will be used consistently across all operations (Google Drive, model download, training)")
            
    except Exception as _ts_e:
        if lang == 'tr':
            print(f"‚ö†Ô∏è Global timestamp sabitleme atlandƒ±: {_ts_e}")
        else:
            print(f"‚ö†Ô∏è Global timestamp setup skipped: {_ts_e}")
    
    # --- Eski Drive timestamp sistemi (uyumluluk i√ßin korundu) ---
    try:
        # Sadece Colab'de anlamlƒ±; fakat kod g√ºvenle √ßalƒ±≈üƒ±r
        from drive_manager import activate_drive_integration
        import yaml as _yaml
        # drive k√∂k√ºn√º config'ten oku
        drive_folder = "SmartFarm/colab_learn/yolo11_models"
        try:
            _cfg_path = 'config_datasets.yaml'
            if os.path.exists(_cfg_path):
                with open(_cfg_path, 'r', encoding='utf-8') as _cf:
                    _cfg = _yaml.safe_load(_cf) or {}
                _gs = _cfg.get('global_settings', {}) if isinstance(_cfg, dict) else {}
                _p = _gs.get('drive_folder_path')
                if isinstance(_p, str) and _p.strip():
                    drive_folder = _p.strip()
        except Exception:
            pass
        dm = activate_drive_integration(folder_path=drive_folder, project_name="yolo11_models")
        if dm and getattr(dm, 'project_folder', None):
            # Global timestamp varsa onu kullan
            if _GLOBAL_TIMESTAMP:
                # DriveManager'ƒ± global timestamp ile senkronize et
                expected_path = os.path.join(os.path.dirname(dm.project_folder), _GLOBAL_TIMESTAMP)
                if dm.project_folder != expected_path:
                    try:
                        # DriveManager'ƒ±n timestamp'ini g√ºncelle
                        dm.project_folder = expected_path
                        os.environ['SMARTFARM_DRIVE_TS'] = expected_path
                        if lang == 'tr':
                            print(f"üîÑ DriveManager global timestamp ile senkronize edildi: {_GLOBAL_TIMESTAMP}")
                        else:
                            print(f"üîÑ DriveManager synchronized with global timestamp: {_GLOBAL_TIMESTAMP}")
                    except Exception:
                        pass
            else:
                # Fallback: eski sistem
                os.environ['SMARTFARM_DRIVE_TS'] = dm.project_folder
    except Exception as _sess_e:
        if lang == 'tr':
            print(f"‚ö†Ô∏è Drive session sabitleme atlandƒ±: {_sess_e}")
        else:
            print(f"‚ö†Ô∏è Drive session setup skipped: {_sess_e}")
    
    # Drive baƒülantƒ± kontrol√º (dil se√ßiminden sonra)
    try:
        from drive_manager import debug_colab_environment, manual_drive_mount
        
        # Colab ortamƒ±nda Drive kontrol√º
        is_colab = debug_colab_environment()
        if is_colab:
            print(f"\n{get_text('drive_check_title', default='üîç Google Drive Baƒülantƒ± Kontrol√º')}")
            print("="*50)
            
            # Drive mount durumu kontrol et
            import os
            if not os.path.exists('/content/drive/MyDrive'):
                print(f"{get_text('drive_not_mounted', default='‚ùå Google Drive mount edilmemi≈ü!')}")
                
                mount_choice = input(f"{get_text('mount_drive_question', default='Drive\'ƒ± ≈üimdi mount etmek ister misiniz? (e/h, varsayƒ±lan: e)')} ").lower() or "e"
                
                if mount_choice.startswith('e'):
                    if manual_drive_mount():
                        print(f"{get_text('drive_mount_success', default='‚úÖ Drive ba≈üarƒ±yla mount edildi!')}")
                    else:
                        print(f"{get_text('drive_mount_failed', default='‚ùå Drive mount ba≈üarƒ±sƒ±z. Eƒüitim yerel kaydetme ile devam edecek.')}")
                else:
                    print(f"{get_text('drive_skip_info', default='‚ÑπÔ∏è Drive mount atlandƒ±. Eƒüitim yerel kaydetme ile yapƒ±lacak.')}")
            else:
                print(f"{get_text('drive_already_mounted', default='‚úÖ Google Drive zaten mount edilmi≈ü!')}")
                
    except ImportError:
        pass  # Drive manager mevcut deƒüilse sessizce devam et
    except Exception as e:
        print(f"‚ö†Ô∏è Drive kontrol hatasƒ±: {e}")
    
    print("\n" + "="*70)
    print(get_text('main_title'))
    print(get_text('main_subtitle'))
    print("="*70)
    
    print(f"\n{get_text('main_menu')}")
    print(get_text('option_download'))
    print(get_text('option_training'))
    print(get_text('option_test'))
    print(get_text('option_exit'))
    
    choice = input(f"\n{get_text('select_option')}") or "1"
    
    if choice == "1":
        download_models_menu()
        if get_text('language_choice').startswith('e'):
            train_now = input("\nEƒüitim kurulumuna ge√ß? (e/h, varsayƒ±lan: e): ").lower() or "e"
            if not train_now.startswith("e"):
                return
        else:
            train_now = input("\nProceed to training setup? (y/n, default: y): ").lower() or "y"
            if not train_now.startswith("y"):
                return
        choice = "2"  # Continue to training
        
    if choice == "2":
        in_colab = is_colab
        
        # (Opsiyonel) Gerekli paketleri y√ºkleme
        # Not: Paket kurulumlarƒ±nƒ± genellikle colab_setup.py √ºzerinden y√∂netmeniz √∂nerilir.
        do_install = (input("\nGerekli paketleri ≈üimdi y√ºklemek ister misiniz? (e/h, varsayƒ±lan: h): ") or "h").lower()
        if do_install.startswith("e"):
            print("\nüì¶ Gerekli paketler y√ºkleniyor...")
            install_required_packages()
        else:
            print("\n‚è≠Ô∏è Paket y√ºkleme atlandƒ±. (colab_setup.py ile kurulumu yapabilirsiniz)")
        
        # Interactive setup - this will handle checkpoint checking
        options = interactive_training_setup()
        if options is None:
            return
        
        # Eƒüitim parametrelerini merkezi olarak normalize et
        options = prepare_training_options(options)
        
        # Check if we're resuming from a checkpoint
        if options.get('resume'):
            print("\n" + "="*50)
            print(f"üîÑ Eƒüitime devam ediliyor: {options['checkpoint_path']}")
            print("="*50)

            # Resume'da veri YAML doƒürulamasƒ±: yoksa Drive'daki checkpoint klas√∂r√ºnden kullan
            try:
                yaml_path = options.get('data', 'merged_dataset.yaml')
                if not os.path.isabs(yaml_path):
                    local_yaml = os.path.join(os.getcwd(), yaml_path)
                else:
                    local_yaml = yaml_path

                if not os.path.exists(local_yaml):
                    ckpt_dir = os.path.dirname(options['checkpoint_path'])
                    drive_yaml = os.path.join(ckpt_dir, os.path.basename(yaml_path))
                    if os.path.exists(drive_yaml):
                        options['data'] = drive_yaml
                        print(f"‚ÑπÔ∏è Yerelde '{yaml_path}' bulunamadƒ±. Drive'dan kullanƒ±lacak: {drive_yaml}")
                    else:
                        print(f"‚ùó Gerekli data YAML bulunamadƒ±: '{yaml_path}'.")
                        print("   - Yerelde yok.")
                        print(f"   - Drive klas√∂r√ºnde de yok: {drive_yaml}")
                        # Kullanƒ±cƒ±ya hƒ±zlƒ± √ß√∂z√ºm: dataset i≈ülemi √ßalƒ±≈ütƒ±rƒ±lsƒ±n mƒ±?
                        do_process = (input("YAML'ƒ± √ºretmek i√ßin veri i≈üleme adƒ±mƒ±nƒ± √ßalƒ±≈ütƒ±ralƒ±m mƒ±? (e/h, varsayƒ±lan: e): ") or "e").lower()
                        if do_process.startswith('e'):
                            dc = options['dataset_config']
                            if dc['type'] == 'hierarchical_multi':
                                if not process_hierarchical_datasets(dc['setup']):
                                    print('‚ùå Veri seti i≈üleme ba≈üarƒ±sƒ±z. √áƒ±kƒ±lƒ±yor...')
                                    return
                                # Ba≈üarƒ±lƒ±ysa yeniden yerel YAML'ƒ± kullan
                                if os.path.exists(local_yaml):
                                    options['data'] = local_yaml
                                    print(f"‚úÖ YAML √ºretildi ve kullanƒ±lacak: {local_yaml}")
                            else:
                                print("‚ö†Ô∏è Bu modda otomatik YAML √ºretimi desteklenmiyor. L√ºtfen 'dataset.yaml' yolunu doƒüru girin.")
                        else:
                            print("‚ùå YAML olmadan eƒüitime devam edilemez. √áƒ±kƒ±lƒ±yor...")
                            return
            except Exception as yaml_check_err:
                print(f"‚ö†Ô∏è Resume √∂ncesi YAML kontrol√ºnde hata: {yaml_check_err}")

            # Skip dataset processing when resuming (YAML doƒürulamasƒ± yapƒ±ldƒ±)
            results = train_model(options, hyp=None, epochs=options['epochs'])
        else:
            # Process dataset(s) for new training
            dataset_config = options['dataset_config']
            
            if dataset_config['type'] == 'single':
                # Single dataset processing (legacy) - API key ve split config ile
                from dataset_utils import download_dataset
                
                api_key = dataset_config.get('api_key')
                split_config = dataset_config.get('split_config')
                
                if not download_dataset(dataset_config['url'], api_key=api_key, split_config=split_config):
                    print('‚ùå Veri seti indirme ba≈üarƒ±sƒ±z. √áƒ±kƒ±lƒ±yor...')
                    return
                    
            elif dataset_config['type'] == 'hierarchical_multi':
                # Hierarchical multi-dataset processing
                if not process_hierarchical_datasets(dataset_config['setup']):
                    print('‚ùå Hiyerar≈üik veri seti i≈üleme ba≈üarƒ±sƒ±z. √áƒ±kƒ±lƒ±yor...')
                    return
                # Se√ßenek 2 ise eƒüitimde config/merged_class_dataset.yaml kullanƒ±lmalƒ±
                try:
                    lm = (dataset_config.get('setup') or {}).get('label_mode')
                    if lm == 'preserve_subclasses':
                        options['data'] = os.path.join('config', 'merged_class_dataset.yaml')
                        print(f"‚ÑπÔ∏è Eƒüitim YAML: {options['data']}")
                except Exception:
                    pass
            
            # Show memory status before training
            show_memory_usage("Eƒüitim √ñncesi")
            
            # Create hyperparameter file for new training
            from hyperparameters import create_hyperparameters_file, load_hyperparameters
            hyp_path = create_hyperparameters_file()
            hyperparameters = load_hyperparameters(hyp_path)
            
            # Kullanƒ±cƒ± tercihlerini kaydet (eƒüitim ba≈ülamadan √∂nce)
            print(f"\nüíæ Kullanƒ±cƒ± tercihleri ve ayarlarƒ± kaydediliyor...")
            augmentation_settings = collect_augmentation_settings(dataset_config)
            configs_dir = save_user_preferences_config(options, dataset_config, augmentation_settings)
            
            if configs_dir:
                # Augmentation ayarlarƒ±nƒ± ayrƒ± dosyada da kaydet
                save_augmentation_config(configs_dir, augmentation_settings)
                
                # Hyperparameter dosyasƒ±nƒ± da configs klas√∂r√ºne kopyala
                try:
                    if hyp_path and os.path.exists(hyp_path):
                        hyp_backup_path = os.path.join(configs_dir, 'hyperparameters_backup.yaml')
                        shutil.copy2(hyp_path, hyp_backup_path)
                        print(f"üìã Hyperparameter dosyasƒ± yedeklendi: {hyp_backup_path}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Hyperparameter yedekleme hatasƒ±: {e}")
            
            # Start new training
            print(f"\nüöÄ Yeni model eƒüitimi ba≈ülatƒ±lƒ±yor...")
            # Normalize edilmi≈ü options zaten mevcut; train_model'e aktar
            results = train_model(options, hyp=hyperparameters, 
                               epochs=options['epochs'], 
                               drive_save_interval=options.get('save_interval', 10))
        
        if results:
            print('‚úÖ Eƒüitim ba≈üarƒ±yla tamamlandƒ±!')
            print(f'üìä Sonu√ßlar: {results}')
            
            # Initialize hierarchical detection if available
            if HIERARCHICAL_DETECTION_AVAILABLE:
                print(f"\nüéØ Hiyerar≈üik tespit sistemi ba≈ülatƒ±lƒ±yor...")
                try:
                    visualizer = HierarchicalDetectionVisualizer()
                    print(f"‚úÖ Hiyerar≈üik tespit sistemi hazƒ±r!")
                    print(f"üè∑Ô∏è  Tespit formatƒ±: 'ZARARLI: Kƒ±rmƒ±zƒ± √ñr√ºmcek (0.85)'")
                except Exception as e:
                    print(f"‚ö†Ô∏è  Hiyerar≈üik tespit ba≈ülatƒ±lamadƒ±: {e}")
            
            # Save to Google Drive (otomatik kaydetme - tekrar soru sorma)
            if in_colab and options.get('drive_save_path'):
                drive_path = options['drive_save_path']
                print(f"\nüíæ Modeller Google Drive'a kaydediliyor...")
                print(f"üìÅ Hedef klas√∂r: {drive_path}")
                if save_models_to_drive(drive_path):
                    print(f"‚úÖ Modeller ba≈üarƒ±yla kaydedildi: {drive_path}")
                    print(f"üìÇ Kaydedilen dosyalar ≈üu konumda: {drive_path}")
                else:
                    print("‚ùå Modeller Google Drive'a kaydedilemedi.")
        else:
            print('‚ùå Eƒüitim ba≈üarƒ±sƒ±z veya kesildi.')
            
            # Save partial results if available (otomatik kaydetme)
            if in_colab and options.get('drive_save_path'):
                drive_path = options['drive_save_path']
                print(f"\nüíæ Kƒ±smi sonu√ßlar Google Drive'a kaydediliyor...")
                print(f"üìÅ Hedef klas√∂r: {drive_path}")
                if save_models_to_drive(drive_path):
                    print(f"‚úÖ Kƒ±smi sonu√ßlar kaydedildi: {drive_path}")
                    print(f"üìÇ Kaydedilen dosyalar ≈üu konumda: {drive_path}")
                else:
                    print("‚ùå Kƒ±smi sonu√ßlar kaydedilemedi.")
        
        # Clean memory
        show_memory_usage("Eƒüitim Sonrasƒ±")
        clean_memory()
    
    elif choice == "3":
        # Test hierarchical detection
        if not HIERARCHICAL_DETECTION_AVAILABLE:
            print("‚ùå Hiyerar≈üik tespit ara√ßlarƒ± mevcut deƒüil.")
            return
        
        model_path = input("Eƒüitilmi≈ü model yolunu girin (varsayƒ±lan: runs/train/exp/weights/best.pt): ").strip() or "runs/train/exp/weights/best.pt"
        if not model_path or not os.path.exists(model_path):
            print("‚ùå Model dosyasƒ± bulunamadƒ±.")
            return
        
        test_image = input("Test g√∂r√ºnt√ºs√º yolunu girin (varsayƒ±lan: test.jpg): ").strip() or "test.jpg"
        if not test_image or not os.path.exists(test_image):
            print("‚ùå Test g√∂r√ºnt√ºs√º bulunamadƒ±.")
            return
        
        try:
            from ultralytics import YOLO
            import cv2
            
            # Load model and visualizer
            model = YOLO(model_path)
            visualizer = HierarchicalDetectionVisualizer()
            
            # Run detection
            print(f"üîç Hiyerar≈üik tespit √ßalƒ±≈ütƒ±rƒ±lƒ±yor...")
            image = cv2.imread(test_image)
            results = model(image)
            
            # Apply hierarchical visualization
            annotated_image = visualizer.process_yolo_results(image, results[0])
            
            # Save result
            output_path = "hierarchical_detection_result.jpg"
            cv2.imwrite(output_path, annotated_image)
            
            # Generate report
            detections = visualizer.get_detection_summary(results[0])
            report = visualizer.create_detection_report(detections)
            
            print(f"\n{report}")
            print(f"‚úÖ A√ßƒ±klamalƒ± g√∂r√ºnt√º kaydedildi: {output_path}")
            
        except Exception as e:
            print(f"‚ùå Hiyerar≈üik tespit testi sƒ±rasƒ±nda hata: {e}")
    
    elif choice == "4":
        print("üëã √áƒ±kƒ±lƒ±yor...")
    
    else:
        print("‚ùå Ge√ßersiz se√ßenek. √áƒ±kƒ±lƒ±yor...")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Kullanƒ±cƒ± tarafƒ±ndan kesildi. √áƒ±kƒ±lƒ±yor...")
    except Exception as e:
        print(f"\n‚ùå Bir hata olu≈ütu: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("\n‚úÖ ƒ∞≈ülem tamamlandƒ±.")
